\lab{Total Variation and Image Processing}{Total Variation and Image Processing}
\label{lab:tv_images}
\objective{Minimizing an energy functional is equivalent to solving the resulting Euler-Lagrange equations.  We introduce the method of steepest descent to solve these equations, and apply this technique to a denoising problem in image processing.}
\labdependencies{AnisotropicDiffusion}


% \begin{enumerate}
% \item Algorithm assumes smoothness.
% Other theorems/algorithms have been developed using convex analysis, due to the recognition that the image might not be best represented by a smooth function $u:[0,1]\times [0,1] \to \mathbb{R}$.
% \item Analyze efficiency.
% Code with cython/numexpr?
% \end{enumerate}

\section*{The Gradient Descent method}
Consider an energy functional $J[u]$, defined over a collection of admissible functions $u:\Omega \subset \mathbb{R}^n \to \mathbb{R}$, with the form
\[J[u] = \int_{\Omega} L(x,u,\nabla u) \, dx\]
where $L = L(x,u,\nabla u)$ is a function $\mathbb{R}^n \times \mathbb{R} \times \mathbb{R}^n \to \mathbb{R}$.
A standard result from the calculus of variations states that a minimizing function $u^*$ satisfies the Euler-Lagrange equation
\begin{align}
L_u-\sum_{i=1}^n\frac{\partial L_{u_{x_i}}}{\partial x_i}=L_u-\nabla\cdot L_{\nabla u}=L_u - {\rm div}\,(L_{\nabla u}) &= 0.    \label{EL:multivar_domain}
\end{align}
where $L_{\nabla u} = \nabla^\prime L = [L_{x_1},\hdots,L_{x_n}]^\intercal$.

This equation is typically an elliptic PDE, possessing boundary conditions associated with  restrictions on the class of admissible functions $u$.
To more easily compute $\eqref{EL:multivar_domain}$, we consider a related parabolic PDE,
\begin{align}
    \begin{split}
    &{ } u_t = -(L_u - {\rm div}\, L_{\nabla u}), \quad t > 0,\\
    &{ } u(x,0) = u_0(x), \quad t = 0.
    \end{split} \label{grad_desc_pde}
\end{align}
A steady state solution of \eqref{grad_desc_pde} does not depend on time, and thus solves the Euler-Lagrange equation.
It is often easier to evolve an initial guess using \eqref{grad_desc_pde}, and stop whenever its steady state is well-approximated, than to solve \eqref{EL:multivar_domain} directly.

\begin{flushleft}
    Consider the energy functional
\end{flushleft}
\[ J[u] = \int_{\Omega} \|\nabla u\|^2 \, dx.\]
% where the class of admissible functions $u$ satisfy appropriate Dirichlet conditions on $\partial \Omega$.
The minimizing function $u^*$ satisfies the Euler-Lagrange equation
\[-{\rm div}\, \nabla u    = - \Delta u = 0.\]
The gradient descent flow is the well-known heat equation
\[u_t = \Delta u.\]

The Euler-Lagrange equation could equivalently be described as $\Delta u = 0$, leading to the PDE $u_t = -\Delta u$.
Since the backward heat equation is ill-posed, it would not be helpful in a search for the steady-state.

Let us take the time to make \eqref{grad_desc_pde} more rigorous.
We recall that
\begin{align*}
        \delta J(u;h) &= \left.\frac{d}{dt}J(u + \epsilon h)\right|_{\epsilon=0},\\
        &= \int_{\Omega} (L_u(u) - {\rm div}\, L_{\nabla u}(u)) h\, dx,\\
        &= \langle L_u(u) - {\rm div}\, L_{\nabla u}(u), h \rangle_{L^2(\Omega)},
\end{align*}
for each $u$ and each admissible perturbation $h$.
Then using the Cauchy-Schwarz inequality,
\begin{align*}
    |\delta J(u;h)| &\leq \|L_y(u) - {\rm div}\, L_{\nabla u}(u)\| \cdot \| h \|
\end{align*}
with equality iff $h =\alpha(L_u(u) - {\rm div}\, L_{\nabla u}(u))$ for some $\alpha \in \mathbb{R}$.
This implies that the ``direction''
$h = L_u(u) - {\rm div}\, L_{\nabla u}(u)$ is the direction of steepest ascent and
maximizes $\delta J(u;h)$.
Similarly,
\[h = -(L_u(u) - {\rm div}\, L_{\nabla u}(u))\]
 points in the direction of steepest descent, and the flow described by \eqref{grad_desc_pde} tends to move toward a state of lesser energy.


\subsection*{Minimizing the area of a surface of revolution}
The area of the surface obtained by revolving a curve $u(x)$ about the $x$-axis is
\[A[u] = \int_a^b 2 \pi u \sqrt{1 + (u')^2} \, dx.
\]
To minimize the functional $A$ over the collection of smooth curves with fixed end points $u(a) = u_a$, $u(b) = u_b$, we use the Euler-Lagrange equation
\begin{align}
    \begin{split}
    0 &= 1 - u \frac{u''}{1 + (u')^2} , \\
    &= 1 + (u')^2 - u u'',
    \end{split}\label{tv_images:SA_EL_equation}
\end{align}
with the gradient descent flow given by
\begin{align}
    \begin{split}
    &{ } u_t = -1 - (u')^2 + u u'',\quad t > 0,\, x \in (a,b), \\
    &{ } u(x,0) = g(x), \quad t = 0,\\
    &{ } u(a,t) = u_a, \quad u(b,t) = u_b.
    \end{split}\label{tv_images:SA_flow}
\end{align}

\subsection*{Numerical Implementation}
We will construct a numerical solution of \eqref{tv_images:SA_flow} using the conditions $u(-1) = 1$, $u(1) = 7$.
A simple solution can be found by using a second-order order discretization in space  with a simple forward Euler step in time. We create the grid and set our end states below.
\begin{lstlisting}
import numpy as np

a, b = -1, 1.
alpha, beta = 1., 7.
####  Define variables x_steps, final_T, time_steps  ####
delta_t, delta_x = final_T/time_steps, (b-a)/x_steps
x0 = np.linspace(a,b,x_steps+1)
\end{lstlisting}

Most numerical schemes have a stability condition that must be satisfied. Our discretization requires that $\frac{\Delta t}{(\Delta x)^2} \leq \frac{1}{2}$.
We continue by checking that this condition is satisfied, and use the straight line connecting the end points as initial data.

\begin{lstlisting}
# Check a stability condition for this numerical method
if delta_t/delta_x**2. > .5:
    print("stability condition fails")

u = np.empty((2,x_steps+1))
u[0]  = (beta - alpha)/(b-a)*(x0-a)  + alpha
u[1] = (beta - alpha)/(b-a)*(x0-a)  + alpha
\end{lstlisting}

Finally, we define the right hand side of our difference scheme, and time step until the scheme converges.
\begin{lstlisting}
def rhs(y):
    # Approximate first and second derivatives to second order accuracy.
    yp = (np.roll(y,-1) - np.roll(y,1))/(2.*delta_x)
    ypp = (np.roll(y,-1) - 2.*y + np.roll(y,1))/delta_x**2.
    # Find approximation for the next time step, using a first order Euler step
    y[1:-1] -= delta_t*(1. + yp[1:-1]**2. - 1.*y[1:-1]*ypp[1:-1])
    return y


# Time step until successive iterations are close
iteration = 0
while iteration < time_steps:
    u[1] = rhs(u[1])
    if norm(np.abs((u[0] - u[1]))) < 1e-5: break
    u[0] = u[1]
    iteration+=1

print("Difference in iterations is ", norm(np.abs((u[0] - u[1]))))
print("Final time = ", iteration*delta_t)
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{figures/min_surface_area.pdf}
\caption{The solution of \eqref{tv_images:SA_EL_equation}, found using the gradient descent flow \eqref{tv_images:SA_flow}.}
\label{fig:tv_images:SA_image}
\end{figure}

\begin{problem}
Using $20$ $x$ steps, $250$ time steps, $a=-1$, $b=1$, $\alpha = 1$, $\beta=7$, and a final time of $0.2$, plot the solution that minimizes \eqref{tv_images:SA_flow}.
It should match figure \ref{fig:tv_images:SA_image}.

\end{problem}

\section*{Image Processing: Denoising}

A greyscale image can be represented by a scalar-valued function $u:\Omega \to \mathbb{R}$, $\Omega \subset \mathbb{R}^2$. The following code reads an image into an array of floating point numbers, adds some noise, and saves the noisy image.
\begin{lstlisting}
from numpy.random import randint, uniform, randn
import matplotlib.pyplot as plt
from matplotlib import cm
from imageio.v3 import imread, imwrite

imagename = 'balloons_color.jpg'
changed_pixels=40000
# Read the image file imagename into an array of numbers, IM
# Multiply by 1. / 255 to change the values so that they are floating point
# numbers ranging from 0 to 1.
IM = imread(imagename, mode='F') * (1. / 255)
IM_x, IM_y = IM.shape

for lost in range(changed_pixels):
    x_,y_ = randint(1,IM_x-2), randint(1,IM_y-2)
    val =  .1*randn() + .5
    IM[x_,y_] = max( min(val,1.), 0.)
imwrite("noised_"+imagename, IM)
\end{lstlisting}
A color image can be represented by three functions $u_1, u_2,$ and $u_3$. In this lab we will work with black and white images, but total variation techniques can easily be used on more general images.

\subsection*{A simple approach to image processing}
Here is a first attempt at denoising: given a noisy image $f$, we look for a denoised image $u$ minimizing the energy functional
\begin{align}
J[u] = \int_{\Omega} L(x,u,\nabla u) \, dx, \label{tv_images:diffusion}
\end{align}
where
\begin{align*}
L(x,u,\nabla u) &= \frac{1}{2}(u-f)^2 + \frac{\lambda}{2} | \nabla u|^2,\\
&= \frac{1}{2}(u-f)^2 + \frac{\lambda}{2} (u_x^2 + u_y^2)^2.
\end{align*}
This energy functional penalizes 1) images that are too different from the original noisy image, and 2) images that have large derivatives. The minimizing denoised image $u$ will balance these two different costs.

Solving for the original denoised image $u$ is a difficult inverse problem-some information is irretrievably lost when noise is introduced. However, a priori information can be used to guess at the structure of the original image.  For example, here $\lambda$ represents our best guess on how much noise was added to the image, and is known as a regularization parameter in inverse problem theory.

The Euler-Lagrange equation corresponding to \eqref{tv_images:diffusion} is
\begin{align*}
L_u - \text{div } L_{\nabla u} &= (u-f) - \lambda \Delta u,\\
&= 0.
\end{align*}
and the gradient descent flow is
\begin{align}
    \begin{split}
u_t &= -(u-f -\lambda \Delta u),\\
u(x,0) &= f(x).
    \end{split} \label{tv_images:diffusion_flow}
\end{align}

Let $u_{ij}^n$ represent our approximation to $u(x_i,y_j)$ at time $t_n$. We will approximate $u_t$ with a forward Euler difference, and $\Delta u$ with centered differences:
\begin{align*}
    u_t &\approx \frac{u_{ij}^{n+1}-u_{ij}^n}{\Delta t},\\
    u_{xx} &\approx \frac{u_{i+1,j}^{n}-2u_{ij}^n + u_{i-1,j}^n}{\Delta x^2}, \\
    u_{yy} &\approx \frac{u_{i,j+1}^{n}-2u_{ij}^n + u_{i,j-1}^n}{\Delta y^2}.
\end{align*}


\begin{problem}
Using $\Delta t = 1e{-3},$ $\lambda = 40,$ $\Delta x = 1,$ and $\Delta y = 1$, implement the numerical scheme mentioned above to obtain a solution $u$. (So $\Omega = [0,n_x]\times [0,n_y]$, where $n_x$ and $n_y$ represent the number of pixels in the $x$ and $y$ dimensions, respectively.) Take 250 steps in time. Plot the image with noise as well as the smoothed image. Compare your results with the first half of Figure \ref{fig:noise_compare_attempts}.

Hint: Use the function \li{np.roll} to compute the spatial derivatives. For example, the second derivative can be approximated at interior grid points using
\begin{lstlisting}
u_xx = np.roll(u,-1,axis=1) - 2*u + np.roll(u,1,axis=1)
\end{lstlisting}
\end{problem}




% diffusion_denoised_baloons_resized_bw.jpg
% \begin{lstlisting}
% delta_t = 1e-3
% lmbda = 40
% u = np.empty((2,IM_x,IM_y))
% u[1] = IM
%
% def laplace(z):
%     # Approximate first and second derivatives to second order accuracy.
%     z_xx = (np.roll(z,-1,axis=0) - 2.*z + np.roll(z,1,axis=0))#/delta_x**2.
%     z_yy = (np.roll(z,-1,axis=1) - 2.*z + np.roll(z,1,axis=1))#/delta_y**2.
%     # Find approximation for the next time step, using a first order Euler step
%     z[1:-1,1:-1] -= delta_t*(   (z[1:-1,1:-1]-IM[1:-1,1:-1])
%                                     -lmbda*(z_xx[1:-1,1:-1] + z_yy[1:-1,1:-1]))
%
% # Iterate towards a steady state solution of the gradient descent flow.
% iteration = 0
% while iteration < time_steps:
%     laplace(u[1])
%     if norm(np.abs((u[0] - u[1]))) < 1e-4: break
%     u[0] = u[1]
%     iteration+=1
%
% \end{lstlisting}
\begin{figure}
\begin{minipage}[b]{.47\linewidth}
\centering
\includegraphics[width=\textwidth]{figures/balloon_gray.pdf}
\caption*{Original image}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.47\linewidth}
\centering
\includegraphics[width=\textwidth]{figures/balloon_noise.pdf}
\caption*{Image with white noise}
\end{minipage}
\caption{Noise.}
\label{fig:noise_firstattempt}
\end{figure}

\begin{figure}
\begin{minipage}[b]{.47\linewidth}
\centering
\includegraphics[width=\textwidth]{figures/balloon_diffusion.pdf}
\caption*{Initial diffusion-based approach}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.47\linewidth}
\centering
\includegraphics[width=\textwidth]{figures/balloon_total_variation.pdf}
\caption*{Total variation based approach}
\end{minipage}
\caption{The solutions of \eqref{tv_images:diffusion_flow} and \eqref{tv_images:tv_flow}, found using a first order Euler step in time and centered differences in space.}
\label{fig:noise_compare_attempts}
\end{figure}


\section*{Image Processing: Total Variation Method}
We represent an image by a function $u:[0,1]\times[0,1] \to \mathbb{R}$.
A $C^1$ function $u:\Omega \to \mathbb{R}$ has bounded total variation on $\Omega$ ($BV(\Omega)$) if $\int_{\Omega} |\nabla u| < \infty$; $u$ is said to have total variation $\int_{\Omega} |\nabla u|$.  Intuitively, the total variation of an image $u$ increases when noise is added.

The total variation approach was originally introduced by Ruding, Osher, and Fatemi\footnote{L. Rudin, S. Osher, and E. Fatemi, ``Nonlinear total variation based noise removal algorithms'', \emph{Physica D.}, 1992.}. It was formulated as follows: given a noisy image $f$, we look to find a denoised image $u$ minimizing
\begin{align}
\int_{\Omega} |\nabla u(x)|\, dx \label{tv_images:tv}
\end{align}
subject to the constraints
\begin{align}
    &{ } \int_{\Omega} u(x) \, dx = \int_{\Omega} f(x)\, dx, \label{tv_images:same_mean}\\
    &{ } \int_{\Omega} |u(x) - f(x)|^2\, dx = \sigma |\Omega|.\label{tv_images:aprior_variance}
\end{align}
Intuitively, \eqref{tv_images:tv} penalizes fast variations in $f$ - this functional together with the constraint \eqref{tv_images:same_mean} has a constant minimum of $u = \frac{1}{|\Omega|}\int_{\Omega} u(x) \, dx$. This is obviously not what we want, so we add a constraint \eqref{tv_images:aprior_variance} specifying how far $u(x)$ is required to differ from the noisy image $f$.  More precisely, \eqref{tv_images:same_mean} specifies that the noise in the image has zero mean, and \eqref{tv_images:aprior_variance} requires that a variable $\sigma$ be chosen a priori to represent the standard deviation of the noise.



Chambolle and Lions proved that the model introduced by Rudin, Osher, and Fatemi can be formulated equivalently as
\begin{align}
F[u] = \min_{u \in BV(\Omega)} \int_{\Omega} |\nabla u| + \frac{\lambda}{2}(u-f)^2 \, dx,
\end{align}
where $\lambda >0$ is a fixed regularization parameter\footnote{A. Chambelle and P.-L. Lions, ``Image recovery via total variation minimization and related problems", \emph{Numer. Math.}, 1997.}. Notice how this functional differs from \eqref{tv_images:diffusion}: $\int_{\Omega} |\nabla u|$ instead of $\int_{\Omega} |\nabla u|^2$. This turns out to cause a huge difference in the result.  Mathematically, there is a nice way to extend $F$ and the class of functions with bounded total variation to functions that are discontinuous across hyperplanes. The term $\int |\nabla|$ tends to preserve edges/boundaries of objects in an image.


% The Euler-Lagrange equation is
% \begin{align*}
% \lambda (u-f) - \frac{u_{xx}u_y^2 + u_{yy}u_x^2 - 2u_xu_yu_{xy}}{(u_x^2 + u_y^2)^{3/2}} &= 0.
% \end{align*}
The gradient descent flow is given by
\begin{align}
    \begin{split}
u_t &= -\lambda (u-f) + \frac{u_{xx}u_y^2 + u_{yy}u_x^2 - 2u_xu_yu_{xy}}{(u_x^2 + u_y^2)^{3/2}} ,\\
u(x,0) &= f(x).
\end{split} \label{tv_images:tv_flow}
\end{align}
Notice the singularity that occurs in the flow when $|\nabla u| = 0$. Numerically we will replace  $|\nabla u|^{3}$ in the denominator with $(\epsilon + |\nabla u|^{2})^{3/2}$, to remove the singularity.


\begin{problem}
Using $\Delta t = 1e-3, \lambda = 1, \Delta x = 1,$ and $ \Delta y = 1$, implement the numerical scheme mentioned above to obtain a solution $u$.  Take 200 steps in time. Display both the diffusion-based and total variaton images of the balloon. Compare your results with Figure \ref{fig:noise_compare_attempts}. How small should $\epsilon$ be?

Hint: To compute the spatial derivatives, consider the following:
\begin{lstlisting}
u_x = (np.roll(u,-1,axis=1) -  np.roll(u,1,axis=1))/2
u_xx = np.roll(u,-1,axis=1) - 2*u + np.roll(u,1,axis=1)
u_xy = (np.roll(u_x,-1,axis=0) - np.roll(u_x,1,axis=0))/2.
\end{lstlisting}
\end{problem}

