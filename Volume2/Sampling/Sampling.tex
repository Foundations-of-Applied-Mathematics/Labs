\lab{Sampling}{Sampling}
\label{lab:Sampling}
\labdependencies{MarkovChains}
\objective{Sampling is an important and fundamental tool in statistical modeling. In this lab we will learn to use PyMC for Bayesian modeling and statistical sampling. This lab will focus on material
from chapters 5 and 6 of Volume 2.}

\section*{Sampling}
In Statistics, a \emph{population} is the collection of all items, groups, or phenomenon we are seeking to study or understand in an experiment.
It is all the data we could possibly have or we wish we had (e.g. all Americans, all NCAA football games).
However, in practice, we rarely have access to the entire population. As such, we must then obtain a finite data set.
\emph{Sampling} is the process where researchers take a predetermined number of draws from a population, called a \emph{sample} (e.g. 750 Americans, 300 NCAA football games).
A good sample can tell us a lot, and while we will not examine what makes a good sample in this lab, we will examine how much a good sample can tell us.
One goal of Bayesian statistics is to be able to quantify, with degrees of certainty, how much we can learn from a sample, given what we already know about its source.
This quantification of certainty allows us to extract more information and nuance from a sample than would otherwise be possible, and in turn allow us to better predict and describe events.

\section*{Parameter Estimation}
Given a sample $\x=x_1,x_2,\ldots,x_n$, we often think of this data as draws or realizations from an independent and identically distributed (i.i.d) sequence of random variables $X_1,X_2,...,X_n$ having the same distribution as some unknown random variable $X$.
This random variable has some distribution (i.e. the pdf/pmf) determined by some parameters that allows us to determine certain measurements about the population like the mean or variance.

A \emph{parameter}, in Statistics, is any measurement that describes a distribution.
Thus, we normally apply some function or formula to the sample in order to obtain information about the distribution that generated the sample.
This function is called a \emph{statistic}.
Specifically, a statistic that is used to estimate a parameter is called an \emph{estimator}, and the result that is obtained when replacing each random variable $X_i$ by each datum $x_i$ is called the \emph{estimate}.

We will examine two methods of parameter estimation: the Frequentist/Classical approach and the Bayesian approach.

\subsection*{Frequentist/Classical Approach: Maximum Likelihood Estimation}
The Frequentist approach views probability as the long-term frequency of an event occurring.
In this approach, we view the parameter as a fixed value that we are trying to estimate.
The estimates we make are known as \emph{point estimates} because they are single values.
\emph{Maximum Likelihood Estimation} (MLE) is a frequentist approach to parameter estimation, viewing probability as the limit of the frequency of success generated by several repeated trials.
The MLE is a method that chooses the parameter for which the sample is most likely to have occurred.

\subsection*{Likelihood}
Finding the maximum likelihood involves, unsurprisingly, maximizing the likelihood function, which is defined as follows
\begin{align*}
  \mathcal{L}(\theta) = \mathcal{L}(\theta | \x) = f(\x | \theta)=\prod_{i=1}^{n}f(x_i| \theta),
\end{align*}
where $\x=x_1,x_2,\ldots,x_n$ is a sample, $\theta$ is the parameter we are estimating and $f$ is the pdf/pmf.
Thus, a MLE of a parameter $\theta$ is the value of $\hat\theta$ that maximizes the likelihood function $\mathcal{L}$.
This value is known as the \emph{maximum likelihood estimate}.
Determining the MLE $\hat{\theta}$ is then as simple as finding the argmax of $\mathcal{L}$
\begin{align*}
  \hat{\theta} =  \argmax_{\theta \in \Theta} \mathcal{L}(\theta | \x).
\end{align*}

\subsection*{Bayesian Approach: Maximum A Posteriori Estimate}
The Bayesian approach views probability as a measure of belief and updates that belief as more evidence is gathered.
We treat the unknown parameter as a random variable and try to compute the distribution of it conditioned on the sample and using some initial belief for the parameter.
If we examine closely, we can see a similarity between the likelihood function and Bayes' rule. Bayes' rule gives the following relation
\begin{align*}
  P(A| B) = \frac{P(B | A)P(A)}{P(B)}.
\end{align*}
In some simple terms, Bayes' rule is framework for updating our ``beliefs" about the hypothesis A given some evidence B.
To apply this rule to the problem of parameter estimation we get the following relation (note $f$ is a pdf/pmf)
\begin{align}\label{eq:cont_bayes}
  f(\theta | \x) = \frac{f(\x | \theta)g(\theta)}{\int_{\Theta}f(\x | \vartheta)g(\vartheta) d\vartheta}.
\end{align}

We call $f(\theta | \x)$ the \emph{posterior distribution}, $f(\x| \theta)$ the \emph{likelihood function}, and $g(\theta)$ the \emph{prior distribution}.
The posterior $f(\theta | \x)$ represents the updated distribution for $\theta$ that takes into account the sample $\x$.
The prior $g(\theta)$ is the initial assumed distribution for $\theta$ that represents our beliefs about $\theta$ before we see the sample, and the likelihood $f(\x | \theta)$ is the function that represents the probability of the sample holding true given the parameter $\theta$.
The denominator is normally called the \emph{marginal likelihood} and is a constant for normalization that represents the probability of generating the sample under any possible value of $\theta\in\Theta$.
As such, the marginal likelikhood is independent of $\theta$.

This approach is in contrast to the Frequentist approach where the parameter is fixed, the sample is a result of various attempts, and we consider no prior knowledge for the parameter.
The Bayesian approach allows us to incorporate prior knowledge into our model, so that we can update our knowledge as we gather more evidence.
But, the result we get is just a probability distribution, not a single value, that tells us which values of $\theta$ are most likely to have produced our sample.

The MAP estimate is then the argmax of the posterior
\begin{align}\label{eq:map}
  \theta_{MAP} = \argmax_{\theta\in\Theta} f(\theta| \x).
\end{align}
When finding the MAP estimate, the exact posterior is often left uncalculated because it is difficult to compute.
Instead, we approximate it by finding its value at grid points of $\theta$.
Similarly, because of the complexity of the denominator, we often don't find it until the end of the process.
After we calculate $f(\x | \theta)g(\theta)$ for each relevant $\theta$ we can then approximate the integral in the denominator with a finite sum.
First we find $f(\x | \theta_i)g(\theta_i)$ for a grid of $\theta$ values.
Then, we can approximate the integral in denominator with the following sum $\frac{1}{n}\sum_{i=1}^n f(\x | \theta_i)g(\theta_i)$ (i.e. using finite sums like Riemann Sums) or using Monte Carlo Methods like MCMC.

Here we can see that the likelihood function is similar to Bayes' rule as long as we take $g(\theta)$ to be a constant, i.e. $\theta\sim \mathcal{U}(a,b)$.
This means that the MAP estimate is the MLE if we assume a uniform prior distribution.

\subsection*{Example 1: Estimating the Lifespan of a Projector Bulb with a Uniform Prior}
Assume that the lifespan of a projector bulb can be modeled as a random variable $X$ with an exponential distribution of unknown parameter $\lambda$.
Suppose that you have a sample of 7 bulbs which lasted
2, 3.3, 4.5, 1.8, 3.1, 2.7, and 2.2 months, respectively. Moreover, assume we have no reason to believe that the lifespan of a bulb is any more likely to be any particular value than any other.
That is,the lifespan can take on any value equally.
Find the posterior pdf for $\lambda$, compute the MAP for $\lambda$, and plot the prior and posterior.

\begin{warn}
    Note that just like NumPy, the stats module of SciPy allows for array broadcasting or vectorization.
    Most, if not all, of the parameters for all of the distributions in the stats module can be ndarrays, and the functions will return an ndarray of the same shape.
    Thus, keep track of shapes so you do not run into errors.
    Lastly, it is important to look at the documentation for each statistical module to understand what parameters are used and if they are different from the ones we or books have described.
\end{warn}

Since $X$ is distributed as an exponential distribution, the likelihood pdf is given by Gamma($1,\lambda$).
Moreover, recall that Gamma$(a,b)$ distribution describes the waiting time for $a>0$ events to occur in a Poisson process with rate $b>0$ (i.e. the time between events).
Some books, modules, and equations, use the parameter \emph{scale}, $\vartheta=\frac{1}{b}$, instead of the rate in the Gamma distribution.\
\li{scipy.stats} is one such module, so we will work $\vartheta=\frac{1}{\lambda}$ as the scale, and then compute the inverse to obtain $\lambda$.

For our example, the event $a$ is the number of bulbs that have failed, and the rate $b$ represents the number of bulb-months at which they fail (i.e.\ bulb-months/bulb-failure).
In our case, we are trying to estimate the rate $\lambda$ at which 1 bulb fails, so $a=1$.
Then, the scale $\vartheta$ represents bulb-failures per bulb-month.
Though $a \in(0,\infty]$ (i.e. we can have any number of bulb failures), we will assume that there are no more than 2 bulb failures per month because the inverse of the sample at least shows that there are no more than 2.
Hence, $a \in(0,2]$.
Though this is a continuous interval and $a$ can take on any value between 0 and 2, we have to discretize the interval for computational purposes since we do not have infinite computational power or memory.
Thus, we assume that there are only 100 possible events.
This implies that there are only 100 possible values for $\vartheta$.

Since the likelihood $f(\x| \theta)=\prod_{i=1}^{n}f(x_i| \theta)$, we can use \li{np.prod()} to calculate the likelihood for each value of $\vartheta$.
Lastly, \li{scipy.stats.uniform} uses the parameter \li{loc} and \li{scale} to create a uniform distribution on the interval [\li{loc}, \li{loc}+\li{scale}].
The normal default values are 1 and 0, respectively.

\begin{lstlisting}
from scipy.stats import uniform, gamma
import matplotlib.pyplot as plt
import numpy as np
>>> sample = 1/np.array([2, 3.3, 4.5, 1.8, 3.1, 2.7, 2.2]) # Sample scale values
>>> fails = np.linspace(0, 2, 100)[1:] # Scale values, not including 0
>>> prior = uniform.pdf(x=fails, loc=0, scale=2)
# List comprehension to prevent broadcasting error
# Compute the product f(x_i|scale) for i=1 to 7 for each value of the scale
>>> likelihood = np.array([(gamma.pdf(x=sample, a=1, scale=fail)).prod() for fail in fails])
>>> integral = (likelihood*prior).sum()*(2/100) # Riemann sums on [0, 2]
>>> posterior = (likelihood*prior)/integral
>>> 1/fails[posterior.argmax()] # MAP estimate
2.6052631578947367

>>> plt.plot(fails, prior, color='orange', label='Prior')
>>> plt.plot(fails, posterior, color='blue', label='Posterior')
>>> plt.ylabel("Probability Density")
>>> plt.xlabel("Scale (bulb-failures/bulb-month)")
>>> plt.legend()
>>> plt.show()
\end{lstlisting}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/bayes_uniform.pdf}
    \caption{The plot of the uniform prior and Gamma posterior distributions for the lifespan of a projector bulb.}
\end{figure}

\begin{problem}\label{prob:unif}
  Write a function called \li{bernoulli_sampling()} that takes the following parameters: \li{p} a float that is the ``fairness" of a coin and \li{n} the number of the Bernoulli trials.
  In this function simulate \li{n} tosses of a coin which gives heads with probability \li{p}.
  Then use that sample to calculate the posterior distribution on $p$ given a uniform prior using Equation \ref{eq:map}.
  Remember that $p$ is the probability parameter, so this is the parameter $\theta$ we are trying to estimate.

  For \li{p}=.2 and \li{n}=100 plot the posterior distribution and return the MAP estimate of $p$, which is also the MLE in this case.
  Be sure to give your plot a relevant title and axis labels.

  Hint: In this case, $f$ is the Binomial pmf $f(x) = p^{n\bar{x}}(1-p)^{n(1-\bar{x})}$.
  You do not need to calculate the integral in the denominator exactly; since you are using a finite approximation of the distributions, you may use a finite approximation of the integral.
  You may simulate the tosses of a coin by using \li{np.random.binomial()} or \li{scipy.stats.binom()}.
  Moreover, you may use \li{scipy.stats.uniform()} to generate the prior distribution, but for this problem it is not necessary given the definition of $\mathcal{U}(a,b)$.
  All of these functions accept a \li{size} parameter, defaulted to 1, that allows you to generate multiple samples at once.
  We will only be using a single sample in this problem so the returned value should be a single integer.
  Refer to \href{https://docs.scipy.org/doc/scipy/tutorial/stats.html}{scipy.stats} for more documentation on discrete and continuous distributions info.
\end{problem}

\subsection*{Non-Uniform Priors}
While we are able to get good estimates, we leave a lot of the power of Bayesian statistics on the table when we only use a uniform prior.
While the uniform prior is free from any preconceptions or biases, it also imparts the least amount of information.
Using a non-uniform prior allows us to actually incorporate prior knowledge or assumptions into our model.
If we have good reason to believe something about a parameter we are exploring before we even draw a sample, we can learn a lot more by accounting for those beliefs.

\subsection*{Example 2: Lifespan of a Projector Bulb with a Non-Uniform Prior}
Consider the same initial set up as Example 1 (i.e. $X\sim\text{Gamma}(1,\lambda)$ and the sample of 7 bulb lives).
But now, we have reason to believe that the lifespan sample originated from a distribution of Gamma$(2,6)$.
Using the prior Gamma$(2,6)$, find the posterior pdf for $\lambda$, compute the MAP for $\lambda$, and plot the prior and posterior.
The code is very similar with the only difference being the prior distribution.

\begin{lstlisting}
>>> prior = gamma.pdf(x=fails, a=2, scale=1/6) # 2 failures in 6 months
>>> likelihood = np.array([(gamma.pdf(x=sample, a=1, scale=fail)).prod() for fail in fails])
>>> integral = (likelihood*prior).sum()*(2/100)
>>> posterior = (likelihood*prior)/integral
>>> 1/fails[posterior.argmax()]
2.9117647058823524

>>> plt.plot(fails, prior, color="orange", label="Prior")
>>> plt.plot(fails, posterior, color="blue", label="Posterior")
>>> plt.ylabel("Probability Density")
>>> plt.xlabel("Scale (bulb-failures/bulb-month)")
>>> plt.legend()
>>> plt.show()
\end{lstlisting}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/bayes_non_uniform.pdf}
    \caption{The plot of the uniform prior and Gamma posterior distributions for the lifespan of a projector bulb. Note that the initial missing piece of the prior is due to the fact we
    only selected 100 grid points for the scale. The graph becomes better with more grid points.}
\end{figure}

Notice how the posterior distribution for the non-uniform prior has a higher probability density and much narrower shape than one given by using a uniform prior.
The MAP estimate is also higher than the uniform prior.
Furthermore, notice how the prior distribution has a similar shape to the posterior distribution.
This is due to \emph{conjugacy}. Conjugacy is a special case where for certain likelihood functions, selecting a specific prior distribution results in the prior and posterior distributions having the same type of distribution.
The prior distribution is then said to be a \emph{conjugate prior} for the likelihood function.
This is not normally seen in most distributions.
But when it does appear, this is a very useful property as it allows us to easily calculate the posterior distribution and prevents us from having to calculate the marginal likelihood or the product of the likelihood and prior.

Overall, remember that the prior distributions represents our initial beliefs about the parameter.
It is important to choose a prior that is consistent with the problem at hand as choosing a poor prior could require you to get more samples and lead to slower calculations and processes.

\begin{comment}
\subsection*{Updates}
Another important tool that is gained in using Bayesian methods is the Bayesian update.
This method is simple but brings important advantages.
The method of the Bayesian update is simply taking the posterior that has already been calculated, and using that as a prior in the next stage.

This method allows us to refine our guesses over time.
We may be in a situation where we are given a limited initial sample, and an estimate is required before a larger sample can be taken.
In these cases, we can compute the posterior given the first sample, then use that posterior as the prior when we receive our next sample.
This allow us to continually improve the confidence in our estimates and distribute that computation over a long period of time.
\end{comment}

\begin{problem}
  Suppose you choose a coin from a bag that produces coins of many weights.
  However, the bag seems to be more likely to produce coins that are strongly biased in favor of heads.
  You're unsure of which kind of coin you've drawn so in order to find out you perform 20 flips.

  Write a function called \li{non_uniform_prior()} that takes the following parameters: \li{p} a float that is the "fairness" of a coin, \li{n} the size of the sample to be generated, and \li{prior} a SciPy distribution object which will act as the prior on $p$.


  Similar to Problem \ref{prob:unif}, simulate \li{n} flips and calculate and plot the posterior distribution (with a title and axis labels).

  Return the MAP estimate.

  Examine the difference in confidence we can have in estimating the bias of the coin if the coin we draw gives heads 90\% of the time as opposed to 40\% of the time.

  Because we think that coins biased in favor of heads are likely, we can choose a prior distribution that matches that assumption.
  In this case we will choose $\text{Beta}(5,1.5)$ as the prior distribution because it gives much more weight to parameters larger than $.5$.
  This is most easily achieved with \li{scipy.stats.beta(5,1.5)} and using the \li{pdf()} method to calculate $g(\theta)$
\end{problem}


\section*{Sampling from a Markov Chain}
A Markov chain is a way to model sequences of states or events.
Markov chains make a few assumptions, one of those being that the probability of each state occurring is dependent only on the previous state.
The relationship between the states are described by what is called a transition matrix.

Markov chains and sampling are like peanut butter and jelly: neither one really lives up to their full potential without the other.
Given the transition matrix of a Markov chain, we can use sampling to better understand what that chain looks and acts like or to get a well-informed idea of what the future may hold.

Sampling from a simple (row stochastic) transition matrix like the one below is as simple as picking a starting state $s_0$, and then using the corresponding row to sample randomly using the probabilities in the row.
\begin{center}
  \begin{tabular}{c c c c}
    & a & b & c\\
    a & 0.7 & 0.1 & 0.2\\
    b & 0.5 & 0.4 & 0.1\\
    c & 0.1 & 0.8 & 0.1
  \end{tabular}
\end{center}
For example, using the above transition matrix, let $s_0=a$, so we will randomly sample from the array $[a,b,c]$ using the respective probabilities $[0.7,0.1,0.2]$.
If the sample gives us $c$, we can set $s_1=c$ and can continue the process to find $s_2,....,s_n$.

\begin{problem}
Given the transition matrix below and assuming the 0th day is sunny, sample from the markov chain to give a possible forecast of the 10 following days.
Return a list of strings, not including the 0th day.
\begin{center}
  \begin{tabular}{c c c c}
    & sun & rain & wind\\
    sun & 0.6 & 0.1 & 0.3\\
    rain & 0.2 & 0.6 & 0.2\\
    wind & 0.3 & 0.4 & 0.3
  \end{tabular}
\end{center}
Hint: \li{np.random.choice()} may be helpful here.
\end{problem}

\section*{PyMC}
Python has many powerful sampling tools including PyMC, an efficient implementation of a method known as Monte Carlo Markov Chain (MCMC) Sampling.
This is a useful technique as it constructs a Markov Chain whose steady state is a probability distribution that is difficult to sample from directly.
Unlike our simple Markov Chain from the last problem, certain Markov Chains are abstract. PyMC gives us a way to work with these more complex scenarios.

\subsection*{Single Variable PyMC}
Consider the following: owners of a restaurant are trying to decide if they should keep selling nachos.
They gather the data for several months about how many people order nachos each day.
One of the owners happened to take a class in Bayesian statistics in college, so she decides test her knowledge.
She assumes the data are distributed as Poisson($\lambda$) for some unknown value of $\lambda$, where $\lambda$ has a prior of Gamma(2,2).
She wishes to solve for $\lambda$ and sets up a PyMC Model for the situation as follows:

\begin{lstlisting}
import numpy as np
import pymc as pm
import arviz as az   # visualization package

with pm.Model() as model:
    # define the prior of lambda as a Gamma(2, 2) distribution
    lam = pm.Gamma('lambda', alpha=2, beta=2)

    # define the likelihood of the data (called nacho_data) to be distributed
    # as Poisson where the expected value of the outcome (mu) is lam
    y = pm.Poisson('y', mu=lam, observed=nacho_data)

    # sample from the posterior
    trace = pm.sample(n)   # n is the desired number of samples
    az.plot_trace(trace)   # plot the posterior and trace plot for lambda

    new_lambda = trace.posterior['lambda']  # trace values of lambda as a list
    mean = float(new_lambda.mean())  # expected value of lambda
\end{lstlisting}

This code generates a model for the prior distribution of $\lambda$, and then incorporates that prior into a model for the Poisson likelihood.
It then samples from the posterior of $\lambda$ \li{n} times, from which we can estimate its expected value.
The function \li{az.plot_trace()} plots both the posterior (on the left) as well as a \emph{trace plot} (on the right),
In each panel, you should see different lines with different colors or linestyles.
These lines represent the different independent chains that were sample, and if the results are significantly different, it may indicate that there is something wrong with the model.
This trace plots indicates how well the sampling converged.
The rule of thumb is: the closer the trace plot resembles a fuzzy caterpillar, the better the Markov Chain converged to the posterior.

\begin{figure}[H]
\centering

\begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/ex1_pymc.pdf}
    \caption{The posterior distribution and trace plot using $\mathcal{U}(0,2)$ for the prior.}
\end{subfigure}

\begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/ex2_pymc.pdf}
    \caption{The posterior distribution and trace plot using Gamma(2,6) for the prior.}
\end{subfigure}

\caption{A comparison of the posterior and trace plots for the lifespan of a projector bulb using a uniform and non-uniform prior, as given in Examples 1 and 2, using PyMC and $n=500$.
Compare to the plots in Figures 1 and 2.}
\end{figure}

We will now reconsider the initial problem of the coin flip.

\begin{problem}
Write a function that accepts the coin flip data in array form and an integer $n$ for the desired number of samples.
Given data that flips a coin 100 times, assume the data are distributed as Bernoulli($p$) for some unknown value of $p$, where $p$ has a prior of Beta(1,1).
Set up a PyMC model for this situation and sample from the posterior $n$ times.
Plot the trace plot and return the expected value of the posterior as a float, \emph{not an array}.

Run the function with data generated by the following code
\begin{lstlisting}
from scipy.stats import bernoulli
data = bernoulli.rvs(0.2, size=100)
\end{lstlisting}
\end{problem}

\subsection*{Multivariate PyMC}
Unlike the Poisson and Bernoulli distributions, many other distributions (including the Normal, Beta, Gamma, and Binomial distributions) have two or more parameters.
These problems can really showcase the usefulness and ease of PyMC.
Multivariate PyMC problems are coded up exactly the same way as the single variable example above, except that now there will be multiple priors defined separately, all of which will be called by the likelihood.

\begin{problem}
Write a function the accepts height data in array form and an integer $n$ for the desired number of samples.
Given a dataset of the measured heights of 100 men, assume the data are distributed as Normal($\mu$, $1/\tau$) where $\mu$ has a prior of Normal($m$, $s$), and $\tau$ has a prior of Gamma($\alpha$, $\beta$).
Your function should have default values \li{m=180}, \li{s=10}, \li{alpha=2}, and \li{beta=10}.
Set up a PyMC model for this situation and sample from the posterior $n$ times.
Plot the trace plots for $\mu$ and $\tau$, and return the expected value of the posterior of $\mu$ as a float, \emph{not as an array}.

Run the function with data generated by the following code
\begin{lstlisting}
heights = np.random.normal(180, 10, 100)
\end{lstlisting}
\noindent Hint: \li{pm.Normal()} uses parameters \li{mu} and either \li{sigma} or \li{tau}, where the variance of the distribution is given by \li{sigma}$^2$ or $1/$\li{tau} respectively.
\end{problem}


\newpage
\section*{Additional Materials}

We will describe some of the most common distributions and what they model. Note $X$ is a random variable and the support is the domain where the pdf/pmf is nonzero.
\begin{itemize}
    \item Discrete Distributions:
    \begin{itemize}
        \item Bernoulli: This is normally used to model the probability of success in a single trial when the outcome can be categorized into exactly one of two categories.
        The support is $\{0,1\}$.
        We normally write $X \sim\text{Bernouilli}(p),\text{ }p$ is the probability of success and is the parameter.
        \item Binomial: This distribution is used to model the number of successes in a fixed number of Bernoulli trials.
        The support is $\{0,1,\ldots,n\}$ for n trials.
        We write $X\sim\text{B}(n,p)$ or $X\sim\text{Binomial}(n,p)$.
        $n$ and $p$ are the parameters.
        \item Poisson: We use this distribution to model the number of occurrences that occur in a fixed interval of time or space.
        The support is $\{1,2,3,\ldots \}$.
        We denote this as $X\sim\text{Poisson}(\lambda)$ where $\lambda$ is the parameter and is an average rate of occurrence.
    \end{itemize}

    \item Continuous Distributions:
    \begin{itemize}
        \item Uniform: This distribution is best used when the every outcome in the sample space $\omega$ is equally likely.
        The support is $[a,b]$, and we denote this as $X\sim\mathcal{U}(a,b)$ where $a$ and $b$ are the parameters.
        \item Normal/Gaussian: This distribution is used to analyze and show data near the mean, and how that is more frequent than data far from the mean.
        The support is $(-\infty,\infty)$. We write $X\sim\mathcal{N}(\mu,\sigma^2)$ where $\mu$ is the mean and $\sigma^2$ is the variance.
        Some books use $\sigma$ as the standard deviation (i.e. the square root of the variance).
        \item Gamma: This describes the waiting time for $a>0$ events to occur in a homogeneous Poisson process with rate $b>0$ (i.e. the time between events).
        The support is $[0,\infty)$.
        We write $X\sim\text{Gamma}(a,b)$ where $a$ is the shape and $b$, the rate, are the parameters.
        The scale is $\frac{1}{b}$, and note sometimes the rate is denoted by $\lambda$.
        The Exponential distribution is a special case of the Gamma distribution where $a=1$.
        \item Chi-Squared is another special case of the Gamma distribution where $a=\frac{n}{2}$ and $b=\frac{1}{2}$ where $n$ is the degrees of freedom.
        \item Beta: This is best used to describe random variables with a range between 0 and 1 since the support is $[0,1]$.
        We write $X\sim\text{Beta}(a,b)$ where $a$ and $b$ are the parameters.
    \end{itemize}
\end{itemize}

Here is an \href{https://medium.com/@ciortanmadalina/overview-of-data-distributions-87d95a5cbf0a}{article} that gives a brief overview of what phenomenon some of the common distributions describe (including the ones we have here).
This other \href{https://towardsdatascience.com/bayesian-ab-testing-part-iv-choosing-a-prior-5a4fe3223bfd}{article} gives a good overview of how to choose a prior distribution for a Bayesian model.

