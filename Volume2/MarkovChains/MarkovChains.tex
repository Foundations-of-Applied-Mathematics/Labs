\lab{Markov Chains}{Markov Chains}
\label{lab:Markov}
\labdependencies{}
\objective{
A Markov chain is a collection of states with specified probabilities for transitioning from one state to another.
They are characterized by the fact that the future behavior of the system depends only on its current state.
In this lab we learn to construct, analyze, and interact with Markov chains, then use a Markov-based approach to simulate natural language.}

\section*{State Space Models} % ===============================================

Many systems can be described by a finite number of \emph{states}.
For example, a board game where players move around the board based on dice rolls can be modeled by a Markov chain.
Each space represents a state, and a player is said to be in a state if their piece is currently on the corresponding space.
In this case, the probability of moving from one space to another only depends on the player's current location; where the player was on a previous turn does not affect their current turn.

Markov chains with a finite number of states have an associated \emph{transition matrix} that stores the information about the possible transitions between the states in the chain.
The $(i,j)$th entry of the matrix gives the probability of moving \textbf{from state $j$ to state $i$}.
Thus, each of the columns of the transition matrix sum to $1$.

\begin{info} % Column stochastic versus row stochastic.
A transition matrix where the columns sum to $1$ is called \emph{column stochastic} (or \emph{left stochastic}).
The rows of a \emph{row stochastic} (or \emph{right stochastic}) transition matrix each sum to $1$ and the $(i,j)$th entry of the matrix is the probability of moving from state $i$ to state $j$.
Both representations are common, but in this lab we exclusively use column stochastic transition matrices for consistency.
\end{info}

Consider a very simple weather model in which the weather tomorrow depends only on the weather today.
For now, we consider only two possible weather states: hot and cold.
Suppose that if today is hot, then the probability that tomorrow is also hot is 0.7, and that if today is cold, the probability that tomorrow is also cold is 0.4.
By assigning ``hot'' to the $0$th row and column, and ``cold'' to the $1$st row and column, this Markov chain has the following transition matrix.

\begin{align*}
\begin{blockarray}{ccc}
& \text{\textcolor{red}{hot today}} & \text{\textcolor{blue}{cold today}} \\
\begin{block}{c[cc]}
\text{\textcolor{red}{hot tomorrow}}   & 0.7 & 0.6 \topstrut \\
\text{\textcolor{blue}{cold tomorrow}} & 0.3 & 0.4 \botstrut \\
\end{block}\end{blockarray}
\end{align*}
%
The $0$th column of the matrix says that if it is hot today, there is a $70\%$ chance that tomorrow will be hot ($0$th row) and a $30\%$ chance that tomorrow will be cold ($1$st row).
The $1$st column says if it is cold today, then there is a $60\%$ chance of heat and a $40\%$ chance of cold tomorrow.

Markov chains can be represented by a \emph{state diagram}, a type of directed graph.
The nodes in the graph are the states, and the edges indicate the state transition probabilities.
The Markov chain described above has the following state diagram.

\begin{figure}[H] % 2-state chain.
\centering
\begin{tikzpicture}[normalcircle/.style={draw, circle, minimum size=1.5cm, thick, node distance=2.5cm}]
    % Place the circles
    \node[normalcircle] (Hot) {};
    \node[red] at (Hot) {hot};

    \node[normalcircle, right=of Hot] (Cold) {};
    \node[blue] at (Cold) {cold};

    % Draw loop, place number, draw line
    \draw[thick,->,>=stealth',red!70!black] (Hot)+(-.52,.52) arc (325:40:.35 and -.85);
    \node[left,red!70!black] at (Hot.west) [shift={+(-.5,0)}] {0.7};
    \draw[thick,->,>=stealth',red!70!black] (Hot.north east) -- node[above] {0.3} (Cold.north west);

    \draw[thick,<-,>=stealth',blue!70!black] (Cold)+(.52,.52) arc (325:40:-.35 and -.85);
    \node[right,blue!70!black] at (Cold.east) [shift={+(.5,0)}] {0.4};
    \draw[thick,->,>=stealth',blue!70!black] (Cold.south west) -- node[below] {0.6} (Hot.south east);
\end{tikzpicture}
\end{figure}

\begin{problem} % MarkovChain.__init__().
\label{prob:markov-chain-class-constructor}
Define a \li{MarkovChain} class whose constructor accepts an $n\times n$ transition matrix $A$ and, optionally, a list of state labels.
If $A$ is not column stochastic, raise a \li{ValueError}.
Construct a dictionary mapping the state labels to the row/column index that they correspond to in $A$ (given by order of the labels in the list), and save $A$, the list of labels, and this dictionary as attributes.
If there are no state labels given, use the labels $\begin{bmatrix}0 & 1 & \ldots & n-1\end{bmatrix}$.

For example, for the weather model described above, the transition matrix is
\begin{align*}
    A = \left[\begin{array}{cc}0.7 & 0.6 \\ 0.3 & 0.4\end{array}\right],
\end{align*}
the list of state labels is \li{["hot", "cold"]}, and the dictionary mapping labels to indices is \li{\{"hot":0, "cold":1\}}.
This Markov chain could be also represented by the transition matrix
\begin{align*}
    \widetilde{A}
    = \left[\begin{array}{cc}
        0.4 & 0.3 \\ 0.6 & 0.7
    \end{array}\right],
\end{align*}
the labels \li{["cold", "hot"]}, and the resulting dictionary \li{\{"cold":0, "hot":1\}}.
\end{problem}

\subsection*{Simulating State Transitions} % ----------------------------------

Simulating the weather model described above requires a programmatic way of choosing between the outgoing transition probabilities of each state.
For example, if it is cold today, we could flip a weighted coin that lands on tails $60\%$ of the time (guess tomorrow is hot) and heads $40\%$ of the time (guess tomorrow is cold) to predict the weather tomorrow.
The \emph{Bernoulli distribution} with parameter $p = 0.4$ simulates this behavior: $60\%$ of draws are $0$, and $40\%$ of draws are a $1$.

A \emph{binomial distribution} is the sum several Bernoulli draws: one binomial draw with parameters $n$ and $p$ indicates the number of successes out of $n$ independent experiments, each with probability $p$ of success.
In other words, $n$ is the number of times to flip the coin, and $p$ is the probability that the coin lands on heads.
Thus, a binomial draw with $n=1$ is a Bernoulli draw.

NumPy does not have a function dedicated to drawing from a Bernoulli distribution; instead, use the more general \li{np.random.binomial()} with $n=1$ to make a Bernoulli draw.

\begin{lstlisting}
>>> import numpy as np

# Draw from the Bernoulli distribution with p = .5 (flip one fair coin).
>>> np.random.binomial(n=1, p=.5)
0                             # The coin flip resulted in tails.

# Draw from the Bernoulli distribution with p = .3 (flip one weighted coin).
>>> np.random.binomial(n=1, p=.3)
0                             # Also tails.
\end{lstlisting}

For the weather model, if the ``cold'' state corresponds to row and column 1 in the transition matrix, $p$ should be the probability that tomorrow is cold.
So, if today is cold, select $p=0.4$; if today is hot, set $p=0.3$.
Then draw from the binomial distribution with $n=1$ and the selected $p$.
If the result is $0$, transition to the ``hot'' state; if the result is $1$, stay in the ``cold'' state.

% Consider again the simple weather model and suppose that today is hot.
% The column that corresponds to ``hot''in the transition matrix is $[0.7, 0.3]$.
% To determine whether tomorrow is hot or cold, draw from the binomial distribution with $n = 1$ and $p = 0.3$.
% If the draw is 1, which has $30\%$ likelihood, then tomorrow is cold.
% If the draw is 0, which has $70\%$ likelihood, then tomorrow is hot.

Using Bernoulli draws to determine state transitions works for Markov chains with two states, but larger Markov chains require draws from a \emph{categorical distribution}, a multivariate generalization of the Bernoulli distribution.
A draw from a categorical distribution with parameters $(p_1,p_2,\ldots,p_k)$ satisfying $\sum_{i=1}^k p_i = 1$ indicates which of $k$ outcomes occurs.
If $k=2$, a draw simulates a coin flip (a Bernoulli draw); if $k=6$, a draw simulates rolling a six-sided die. %, like rolling a weighted $k$-sided die.
Just as the Bernoulli distribution is a special case of the binomial distribution, the categorical distribution is a special case of the \emph{multinomial distribution} which indicates how many times each of the $k$ outcomes occurs in $n$ repeated experiments.
Use \li{np.random.multinomial()} with $n = 1$ to make a categorical draw.

\begin{lstlisting}
# Draw from the categorical distribution (roll a fair four-sided die).
>>> np.random.multinomial(1, np.array([1./4, 1./4, 1./4, 1./4]))
array([0, 0, 0, 1])     # The roll resulted in a 3.

# Draw from another categorical distribution (roll a weighted four-sided die).
>>> np.random.multinomial(1, np.array([.5, .3, .2, 0]))
array([0, 1, 0, 0])     # The roll resulted in a 1.
\end{lstlisting}

Consider a four-state weather model with the transition matrix
\begin{comment} % The beginnings of the 4-state chain diagram.
\begin{tikzpicture}[normalcircle/.style={draw,circle,minimum size=1.5cm,fill=none,thick,node distance=3.5cm}]
% Nodes
\node[normalcircle] (A) {\textcolor{red}{hot}};
\node[normalcircle] (B) [above of=A] {\textcolor{green}{mild}};
\node[normalcircle] (C) [right of=B] {\textcolor{blue}{cold}};
\node[normalcircle] (D) [below of=C] {\textcolor{cyan}{freezing}};
% Edges
\draw[bend right=60,thick,->,>=stealth'] (D) edge (C);
\draw[bend left=60,thick,->,>=stealth',blue] (B) edge (C);
\draw[bend right=60,thick,->,>=stealth',blue] (B) edge (A);
\foreach \a/\b in {A/B,A/C,A/D,C/B,C/D} \draw[thick,->,>=stealth'] (\a) edge (\b);
\draw[thick,->,>=stealth',blue] (B) edge (D);
\draw[thick,->,>=stealth',shorten >=1pt,blue] (B) to [out=110,in=170,loop,looseness=4.5] (B);
\end{tikzpicture}
\end{comment}
\begin{align*}
\begin{blockarray}{ccccc}
& \text{\textcolor{red}{hot}} & \text{\textcolor[rgb]{0,.6,0}{mild}} & \text{\textcolor{blue}{cold}} & \text{\textcolor{cyan}{freezing}} \\
\begin{block}{c[cccc]}
\text{\textcolor{red}{hot}}              & 0.5 & 0.3 & 0.1 & 0   \topstrut \\
\text{\textcolor[rgb]{0,.6,0}{mild}}     & 0.3 & 0.3 & 0.3 & 0.3 \\
\text{\textcolor{blue}{cold}}            & 0.2 & 0.3 & 0.4 & 0.5 \\
\text{\textcolor{cyan}{freezing}}        &   0 & 0.1 & 0.2 & 0.2 \botstrut \\
\end{block}\end{blockarray}.
\end{align*}
If today is hot, the probabilities of transitioning to each state are given by the ``hot'' column of the transition matrix.
Therefore, to choose a new state, draw from the categorical distribution with parameters ($0.5$, $0.3$, $0.2$, $0$).
The result $\begin{bmatrix}0 & 1 & 0 & 0\end{bmatrix}$ indicates a transition to the state corresponding to the $1$st row and column (tomorrow is mild), while the result $\begin{bmatrix}0 & 0 & 1 & 0\end{bmatrix}$ indicates a transition to the state corresponding to the $2$nd row and column (tomorrow is cold).
In other words, the position of the $1$ tells which column of the matrix to use as the parameters for the next categorical draw.

\begin{problem} % MarkovChain.transition()
Write a method for the \li{MarkovChain} class that accepts a single state label.
Use the label-to-index dictionary to determine the column of $A$ that corresponds to the provided state label, then draw from the corresponding categorical distribution to choose a state to transition to.
Return the corresponding label of the new state (not its index) as a string.
\\(Hint: \li{np.argmax()} may be useful.)
\label{prob:markov-class-transition}
\end{problem}

\begin{problem} % MarkovChain.walk() and MarkovChain.path().
\label{prob:markov-class-walkers}
Add the following methods to the \li{MarkovChain} class.
\begin{itemize}
\item \li{walk()}: Accept a state label and an integer $N$.
Starting at the specified state, use your method from Problem \ref{prob:markov-class-transition} to transition from state to state $N-1$ times, recording the state label at each step.
Return the list of $N$ state labels (as strings), including the initial state.
\item \li{path()}: Accept labels for an initial state and an end state.
Beginning at the initial state, transition from state to state until arriving at the specified end state, recording the state label at each step.
Return the list of state labels (as strings), including the initial and final states.
\end{itemize}
Test your methods on the two-state and four-state weather models described previously.
\end{problem}

\section*{General State Distributions} % ======================================

\begin{comment} % The 1-Norm. Might be able to get away without this.
For an $n\times 1$ vector $\x$ with entries $x_i$ and an $n\times n$ matrix $A$ with entries $a_{ij}$, the \emph{1-norm} is defined as follows.
\begin{align*}
\|\x\|_1 = \sum_{i=1}^n|x_i| && \|A\|_1 = \sup_j \sum_{i=1}^n |a_{ij}|
\end{align*}
In other words, the $1$-norm for both vectors and matrices is the maximum absolute column sum.
Then if $A$ is a transition matrix, $\|A\|_1 = 1$, since each of the entries of the matrix are positive and each of the columns sum to $1$ by definition.
The power method with the 1-norm can be used to find the unique stable steady state distribution of $A$.
\end{comment}

For a Markov chain with $n$ states, the probability of being in each state can be encoded by a $n$-vector $\x$, called a \emph{state distribution vector}.
The entries of $\x$ must be nonnegative and sum to 1, and the $i$th entry $x_i$ of $\x$ is the probability of being in state $i$.
For example, the state distribution vector $\x = \begin{bmatrix}0.8 & 0.2\end{bmatrix}\trp$ corresponding to the 2-state weather model indicates an $80\%$ chance that today is hot and a $20\%$ chance that today is cold.
On the other hand, the vector $\x = \begin{bmatrix}0 & 1\end{bmatrix}\trp$ implies that today is, with $100\%$ certainty, cold.

If $A$ is a transition matrix for a Markov chain with $n$ states and $\x$ is a corresponding state distribution vector, then $A\x$ is also a state distribution vector.
% To verify this fact, let $a_{ij}$ be the entries of $A$ and $x_i$ the entries of $\x$.
% The columns of $A$ sum to $1$, so $\sum_{j=1}^n a_{ij} = 1$ for $i = 1,\ 2,\ \ldots,\ n$.
% In addition, $\sum_{j=1}^n x_j = 1$ since the entries of $\x$ also sum to $1$.
% From matrix multiplication, the $i$th entry of $A\x$ is given by $\sum_{j=1}^n a_{ij}x_j$, so the sum of the entries of $A\x$ is
% \[\sum_{i=1}^n\sum_{j=1}^n a_{ij}x_j
% = \sum_{j=1}^n\left(x_j\left(\sum_{j=1}^n a_{ij}\right)\right)
% = \sum_{j=1}^n x_j = 1.\]
In fact, if $\x_k$ is the state distribution vector corresponding to a certain time $k$, then $\x_{k+1} = A\x_k$ contains the probabilities of being in each state after allowing the system to transition again.
For the weather model, this means that if there is an $80\%$ chance that it will be hot 5 days from now, written $\x_{5} = \begin{bmatrix}0.8 & 0.2\end{bmatrix}\trp$, then since
\[
\x_{6} = A\x_{5} =
\left[\begin{array}{cc}
0.7 & 0.6 \\
0.3 & 0.4 \\
\end{array}\right]
\left[\begin{array}{c}0.8 \\ 0.2\end{array}\right]
=
\left[\begin{array}{c}0.68 \\ 0.32\end{array}\right],
\]
there is a $68\%$ chance that 6 days from now will be a hot day.

\subsection*{Convergent Transition Matrices} % --------------------------------

Given an initial state distribution vector $\x_{0}$, defining $\x_{k+1} = A\x_k$ yields the significant relation
\[
\x_k = A\x_{k-1} = A(A\x_{k-2}) = A(A(A\x_{x-3})) = \cdots = A^k\x_{0}.
\]

This indicates that the $(i,j)$th entry of $A^k$ is the probability of transition from state $j$ to state $i$ in $k$ steps.
For the transition matrix of the 2-state weather model, a pattern emerges in $A^k$ for even small values of $k$:
\[
A = \left[\begin{array}{cc}
0.7 & 0.6 \\
0.3 & 0.4 \\
\end{array}\right],
\quad
A^2 = \left[\begin{array}{cc}
0.67 & 0.66 \\
0.33 & 0.34 \\
\end{array}\right],
\quad
A^3 = \left[\begin{array}{cc}
0.667 & 0.666 \\
0.333 & 0.334 \\
\end{array}\right].
\]
As $k\rightarrow\infty$, the entries of $A^k$ converge, written
\begin{equation}
\lim_{k \rightarrow \infty} A^k = \left[\begin{array}{ccc}
2/3 & 2/3 \\
1/3 & 1/3 \\
\end{array}\right].
\label{eq:markov-steady-transition}
\end{equation}
In addition, for any initial state distribution vector $\x_{0} = [a,\ b]\trp$ (meaning $a,b\ge 0$ and $a + b = 1$),
\[
\lim_{k \rightarrow \infty} \x_k = \lim_{k \rightarrow \infty}A^k\x_{0}
=
\left[\begin{array}{ccc}
2/3 & 2/3 \\
1/3 & 1/3 \\
\end{array}\right]
\left[\begin{array}{c}a\\b\end{array}\right]
=
\left[\begin{array}{c}2(a+b)/3\\(a+b)/3\end{array}\right]
=
\left[\begin{array}{c}2/3\\1/3\end{array}\right].
\]

Thus, $\x_k \rightarrow \x = \begin{bmatrix}2/3 & 1/3\end{bmatrix}\trp$ as $k\rightarrow\infty$,  regardless of the initial state distribution $\x_{0}$.
So, according to this model, no matter the weather today, the probability that it is hot a week from now is approximately $66.67\%$.
In fact, approximately 2 out of 3 days in the year should be hot.

\subsection*{Steady State Distributions} % ------------------------------------

The state distribution $\x = \begin{bmatrix}2/3 & 1/3\end{bmatrix}\trp$ has another important property:
\[
A\x =
\left[\begin{array}{cc}
7/10 & 3/5 \\
3/10 & 2/5 \\
\end{array}\right]
\left[\begin{array}{c}2/3 \\ 1/3\end{array}\right]
=
\left[\begin{array}{c}14/30 + 3/15 \\ 6/30 + 2/15\end{array}\right]
=
\left[\begin{array}{c}2/3 \\ 1/3\end{array}\right]
= \x.
\]
Any $\x$ satisfying $A\x = \x$ is called a \emph{steady state distribution} or a \emph{stable fixed point} of $A$.
In other words, a steady state distribution is an eigenvector of $A$ corresponding to the eigenvalue $\lambda = 1$.

% TODO: Verify this paragraph.
Every finite Markov chain has at least one steady state distribution.
If some power $A^k$ of $A$ has all positive (nonzero) entries, then the steady state distribution is unique.%
\footnote{This is a consequence of the \emph{Perron-Frobenius theorem}, which is presented in detail in Volume 1.}
In this case, $\lim_{k\rightarrow\infty}A^k$ is the matrix whose columns are all equal to the unique steady state distribution, as in (\ref{eq:markov-steady-transition}).
Under these circumstances, the steady state distribution $\x$ can be found by iteratively calculating $\x_{k+1} = A\x_k$, as long as the initial vector $\x_{0}$ is a state distribution vector.

\begin{warn}
Though every Markov chain has at least one steady state distribution, the procedure described above fails if $A^k$ fails to converge.
For instance, consider the transition matrix
\[
A = \left[\begin{array}{ccc}
0 & 0 & 1 \\
0 & 1 & 0 \\
1 & 0 & 0
\end{array}\right]
,\quad A^k = \begin{cases}
A\quad\text{if } k\ \text{is odd}
\\
I\quad\text{if } k\ \text{is even.}
\end{cases}
\]
In this case as $k\rightarrow\infty$, $A^k$ oscillates between two different matrices.

Furthermore, the steady state distribution is not always unique; the transition matrix defined above, for example, has infinitely many.
\end{warn}

\begin{problem} % Use the power method (simple) to get the steady state.
Write a method for the \li{MarkovChain} class that accepts a convergence tolerance \li{tol} and a maximum number of iterations \li{maxiter}.
Generate a random state distribution vector $\x_{0}$ and calculate $\x_{k+1} = A\x_k$ until $\|\x_{k+1} - \x_k\|_1 <$ \li{tol}, where $A$ is the transition matrix saved in the constructor.
If $k$ exceeds \li{maxiter}, raise a \li{ValueError} to indicate that $A^k$ does not converge.
Return the approximate steady state distribution $\x$ of $A$.

To test your function, generate a random transition matrix $A$.
Verify that $A\x = \x$ and that the columns of $A^k$ approach $\x$ as $k\rightarrow\infty$.
To compute $A^k$, use NumPy's (very efficient) algorithm for computing matrix powers. % (which is not part of \li{scipy.linalg}).

\begin{lstlisting}
>>> A = np.array([[.7, .6],[.3, .4]])
>>> np.linalg.matrix_power(A, 10)       # Compute A^10.
array([[ 0.66666667,  0.66666667],
       [ 0.33333333,  0.33333333]])
\end{lstlisting}
%
Finally, use your method to validate the results of Problem \ref{prob:markov-class-walkers}:
for the two-state and four-state weather models,
\begin{enumerate}
    \item Calculate the steady state distribution corresponding to the transition matrix.
    \item Run a weather simulation for a large number of days using \li{walk()} and verify that the results match the steady state distribution (for example, approximately 2/3 of the days should be hot for the two-state model).
\end{enumerate}
\label{prob:markov-power-method}
\end{problem}

\begin{info}
Problem \ref{prob:markov-power-method} is a special case of the \emph{power method}, an algorithm for calculating an eigenvector of a matrix corresponding to the eigenvalue of largest magnitude.
The general power method, together with a discussion of its convergence conditions, is discussed in Volume 1.
\end{info}

\section*{Using Markov Chains to Simulate English} % ==========================

One of the original applications of Markov chains was to study \emph{natural languages}, meaning spoken or written languages like English or Russian \cite{von2006five}.
In the early $20$th century, Markov used his chains to model how Russian switched from vowels to consonants.
By mid-century, they had been used as an attempt to model English.
It turns out that plain Markov chains are, by themselves, insufficient to model or produce very good English.
However, they can approach a fairly good model of bad English, with sometimes amusing results.

By nature, a Markov chain is only concerned with its current state, not with previous states.
A Markov chain simulating transitions between English words is therefore completely unaware of context or even of previous words in a sentence.
For example, if a chain's current state is the word ``continuous,'' the chain may say that the next word in a sentence is more likely to be ``function'' rather than ``raccoon.''
However the phrase ``continuous function'' may be gibberish in the context of the rest of the sentence.

% The transition probabilities of the resulting Markov chain should reflect the sort of English that the source authors speak.
% Thus, the Markov chain built from \emph{The Complete Works of William Shakespeare} differs greatly from the Markov chain built from a collection of academic journals.
% The source collection of works in the next problems is called the \emph{training set}.

\subsection*{Generating Random Sentences} % -----------------------------------

Consider the problem of generating English sentences that are similar to the text contained in a specific file, called the \emph{training set}.
The goal is to construct a Markov chain whose states and transition probabilities represent the vocabulary and---hopefully---the style of the source material.
There are several ways to approach this problem, but one simple strategy is to assign each unique word in the training set to a state, then construct the transition probabilities between the states based on the ordering of the words in the training set.
To indicate the beginning and end of a sentence requires two extra states: a \emph{start state}, \textcolor[rgb]{0,.6,0}{\$tart}, marking the beginning of a sentence; and a \emph{stop state}, \textcolor{red}{\$top}, marking the end.
The start state should only transition to words that appear at the beginning of a sentence in the training set, and only words that appear at the end a sentence in the training set should transition to the stop state.

% English sentences are of varying length, so it is unknown beforehand how many words to choose (how many state transitions to make) before ending the sentence.
% To capture this feature, include two extra states in the chain: a \emph{start state}, \textcolor[rgb]{0,.6,0}{\$tart}, marking the beginning of a sentence; and a \emph{stop state}, \textcolor{red}{\$top}, marking the end.
% Thus, a training set with $n$ unique words has an $(n+2)\times (n+2)$ transition matrix.

Consider the following small training set, paraphrased from Dr. Seuss \cite{geisel1960green}.

\begin{lstlisting}
<<I am Sam Sam I am.
Do you like green eggs and ham?
I do not like them, Sam I am.
I do not like green eggs and ham.>>
\end{lstlisting}

There are 15 unique words in this training set, including punctuation (so ``ham?'' and ``ham.'' are counted as distinct words) and capitalization (so ``Do'' and ``do'' are also different):
%
\begin{align*}
\text{I\quad am\quad Sam\quad am.\quad Do\quad you\quad like\quad green}
\\
\text{eggs\quad and\quad ham?\quad do\quad not\quad them,\quad ham.}
\end{align*}
%
With start and stop states, the transition matrix should be $17 \times 17$.
Each state must be assigned a row and column index in the transition matrix, for example,
% An easy way to do this is to assign the states an index based on the order that they appear in the training set.
%
\begin{align*}
\begin{array}{ccccccc}
\text{\textcolor[rgb]{0,.6,0}{\$tart}} & \text{I} & \text{am} & \text{Sam} & \ldots & \text{ham.} & \text{\textcolor{red}{\$top}}
\\
0 & 1 & 2 & 3 & \ldots & 15 & 16
\end{array}
\end{align*}
%
The $(i,j)$th entry of the transition matrix $A$ should be the probability that word $j$ is followed by word $i$.
For instance, the word ``Sam'' is followed by the words ``Sam'' once and ``I'' twice in the training set, so the state corresponding to ``Sam'' (index 3) should transition to the state for ``Sam'' with probability $1/3$, and to the state for ``I'' (index 1) with probability $2/3$.
That is, $A_{3,3} = 1/3$, $A_{1,3} = 2/3$, and $A_{i,3} = 0$ for $i \notin\{1,3\}$.
Similarly, the start state should transition to the state for ``I'' with probability $3/4$, and to the state for ``Do'' with probability $1/4$; the states for ``am.'', ``ham?'', and ``ham.'' should each transition to the stop state.

To construct the transition matrix, parse the training set and add $1$ to $A_{i,j}$ every time word $j$ is followed by word $i$, in this case arriving at the matrix
%
\begin{align*}
\begin{blockarray}{cccccccc}
& \text{\textcolor[rgb]{.3,.6,.1}{\$tart}} & \textcolor{gray}{\text{I}} & \textcolor{gray}{\text{am}} & \textcolor{gray}{\text{Sam}} & & \textcolor{gray}{\text{ham.}} & \text{\textcolor[rgb]{1,0,0}{\$top}} \\
\begin{block}{c[ccccccc]}
\text{\textcolor[rgb]{.3,.6,.1}{\$tart}}    & 0 & 0 & 0 & 0 & \ldots & 0 & 0 \topstrut \\
\textcolor{gray}{\text{I}}        & 3 & 0 & 0 & 2 & \ldots & 0 & 0\\
\textcolor{gray}{\text{am}}       & 0 & 1 & 0 & 0 & \ldots & 0 & 0\\
\textcolor{gray}{\text{Sam}}      & 0 & 0 & 1 & 1 & \ldots & 0 & 0\\
& \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
\textcolor{gray}{\text{ham.}}     & 0 & 0 & 0 & 0 & \ldots & 0 & 0\\
\text{\textcolor[rgb]{1,0,0}{\$top}}        & 0 & 0 & 0 & 0 & \ldots & 1 & 0 \botstrut \\
\end{block}\end{blockarray}.
\end{align*}
To avoid a column of zeros, set $A_{j,j} = 1$ where j is the index of the stop state (so the stop state always transitions to itself).
Next, divide each column by its sum so that each column sums to 1:
\begin{align*}
\begin{blockarray}{cccccccc}
& \text{\textcolor[rgb]{.3,.6,.1}{\$tart}} & \textcolor{gray}{\text{I}} & \textcolor{gray}{\text{am}} & \textcolor{gray}{\text{Sam}} & & \textcolor{gray}{\text{ham.}} & \text{\textcolor[rgb]{1,0,0}{\$top}} \\
\begin{block}{c[ccccccc]}
\text{\textcolor[rgb]{.3,.6,.1}{\$tart}} & 0 & 0 & 0 & 0 & \ldots & 0 & 0 \topstrut\\
\textcolor{gray}{\text{I}}        & 3/4 & 0 & 0 & 2/3 & \ldots & 0 & 0\\
\textcolor{gray}{\text{am}}       & 0 & 1/5 & 0 & 0 & \ldots & 0 & 0\\
\textcolor{gray}{\text{Sam}}      & 0 & 0 & 1 & 1/3 & \ldots & 0 & 0\\
& \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
\textcolor{gray}{\text{ham.}}     & 0 & 0 & 0 & 0 & \ldots & 0 & 0\\
\text{\textcolor[rgb]{1,0,0}{\$top}}    & 0 & 0 & 0 & 0 & \ldots & 1 & 1 \botstrut\\
\end{block}\end{blockarray}.
\end{align*}

The $3/4$ indicates that 3 out of 4 times, the sentences in the training set start with the word ``I''.
Similarly, the $2/3$ and $1/3$ says that ``Sam'' is followed by ``I'' twice and by ``Sam'' once in the training set.
Note that ``am'' (without a period) always transitions to ``Sam'' and that ``ham.'' (with a period) always transitions the stop state.

The entire procedure of creating the transition matrix for the Markov chain with words from a file as states is summarized below.

\begin{algorithm}[H] % Read a file and convert it into a Markov chain.
\begin{algorithmic}[1]
\Procedure{MakeTransitionMatrix}{\texttt{filename}}
\State Read the training set from the file \texttt{filename}.
\State Get the set of unique words in the training set (the state labels).
\State Add labels \li{"\$tart"} and \li{"\$top"} to the set of states labels.
\State Initialize an appropriately sized square array of zeros to be the transition matrix.
\For {each sentence in the training set}
    \State Split the sentence into a list of words.
    \State Prepend \li{"\$tart"} and append \li{"\$top"} to the list of words.
    \For {each consecutive pair $(x, y)$ of words in the list of words}
    \State Add 1 to the entry of the transition matrix that corresponds to \par\qquad\qquad transitioning from state $x$ to state $y$.
    \EndFor
\EndFor
\State Make sure the stop state transitions to itself.
\State Normalize each column by dividing by the column sums.
\EndProcedure
\end{algorithmic}
\caption{Convert a training set of sentences into a Markov chain.}
\label{alg:MarkovSentencesTransitionMatrix}
\end{algorithm}

\begin{problem} % SentenceGenerator.__init__()
\label{prob:markov-random-sentences-init}
Write a class called \li{SentenceGenerator} that inherits from the \li{MarkovChain} class.
The constructor should accept a filename (the training set).
Read the file and build a transition matrix from its contents as described in Algorithm \ref{alg:MarkovSentencesTransitionMatrix}.
Save the same attributes as the constructor of \li{MarkovChain} does so that inherited methods work correctly.
Assume that the training set has one complete sentence written on each line.
\\(Hint: if the contents of the file are in the string \li{s}, then \li{s.split()} is the list of words and \li{s.split('\n')} is the list of sentences.)
\end{problem}

\begin{info} % The steady state is the stop state.
The Markov chains that result from the procedure in Problem \ref{prob:markov-random-sentences-init} have a few interesting structural characteristics.
The stop state is a \emph{sink}, meaning it only transitions to itself.
Because of this, and since every node has a path to the stop state, any traversal of the chain will end up in the stop state forever.
The stop state is therefore called an \emph{absorbing state}, and the chain as a whole is called an \emph{absorbing Markov chain}.
Furthermore, the steady state is the vector with a $1$ in the entry corresponding to the stop state and $0$s everywhere else.
\end{info}

\begin{problem} % Create random sentences.
\label{prob:markov-random-sentences-babble}
Add a method to the \li{SentenceGenerator} class called \li{babble()}.
Use the \li{path()} method from Problem \ref{prob:markov-class-walkers} to generate a random sentence based on the training document.
That is, generate a path from the start state to the stop state, remove the \li{"\$tart"} and \li{"\$top"} labels from the path, and join the resulting list together into a single, space-separated string. Return this string.

For example, your \li{SentenceGenerator} class should be able to create random sentences that sound somewhat like Yoda speaking.

\begin{lstlisting}
>>> yoda = SentenceGenerator("yoda.txt")
>>> for _ in range(3):
... 	print(yoda.babble())
...
<<Impossible to my size, do not!
For eight hundred years old to enter the dark side of Congress there is.
But beware of the Wookiees, I have.>>
\end{lstlisting}
\end{problem}

\newpage

\section*{Additional Material} % ==============================================

\subsection*{Other Applications of Markov Chains} % ---------------------------

Markov chains are a useful way to study many probabilistic phenomena, so they have a wide variety of applications.
The following are just a few that are covered in other parts of this lab manual series.
\begin{itemize}
\item \textbf{PageRank}: Google's PageRank algorithm uses a Markov chain-based approach to rank web pages.
The main idea is to use the entries of the steady state vector as a measure of importance for the corresponding states.
For example, the steady state $\x = \begin{bmatrix}2/3 & 1/3\end{bmatrix}\trp$ for the two-state weather model means that the hot state is ``more important'' (occurs more frequently) than the cold state.
See the PageRank lab in Volume 1.

\item \textbf{MCMC Sampling}: A \emph{Monte Carlo Markov Chain} (MCMC) method constructs a Markov chain whose steady state is a probability distribution that is difficult to sample from directly.
This provides a way to sample from nontrivial or abstract distributions.
Many MCMC methods are used in various fields, from machine learning to physics.
See the Volume 3 lab on the Metropolis-Hastings algorithm.

\item \textbf{Hidden Markov Models}: The Markov chain simulations in this lab use an initial condition (a state distribution vector $\x_0$) and known transition probabilities to make predictions forward in time.
Conversely, a \emph{hidden Markov model} (HMM) assumes that a given set of observations are the result of a Markov process, then uses those observations to infer the corresponding transition probabilities.
Hidden Markov models are used extensively in modern machine learning, especially for speech and language processing.
See the Volume 3 lab on Speech Recognition.
\end{itemize}

\subsection*{Large Training Sets} % -------------------------------------------

The approach in Problems \ref{prob:markov-random-sentences-init} and \ref{prob:markov-random-sentences-babble} begins to fail as the training set grows larger.
For example, a single Shakespearean play may not be large enough to cause memory problems, but \emph{The Complete Works of William Shakespeare} certainly will.

To accommodate larger data sets, consider use a sparse matrix from \li{scipy.sparse} for the transition matrix instead of a regular NumPy array.
Specifically, construct the transition matrix as a \li{lil_array} (which is easy to build incrementally), then convert it to the \li{csc_array} format (which supports fast column operations).
Ensure that the process still works on small training sets, then proceed to larger training sets.
How are the resulting sentences different if a very large training set is used instead of a small training set?

\subsection*{Variations on the English Model} % -------------------------------

Choosing a different state space for the English Markov model produces different results.
Consider modifying the \li{SentenceGenerator} class so that it can determine the state space in a few different ways.
The following ideas are just a few possibilities.

\begin{itemize}
\item Let each punctuation mark have its own state.
In the Dr. Seuss training set, instead of having two states for the words ``ham?'' and ``ham.'', there would be three states: ``ham'', ``?'', and ``.'', with ``ham'' transitioning to both punctuation states.

\item Model paragraphs instead of sentences.
Add a \textcolor[rgb]{0,.6,0}{\$tartParagraph} state that always transitions to \textcolor[rgb]{0,.6,0}{\$tartSentence} and a \textcolor{red}{\$topParagraph} state that is sometimes transitioned to by \textcolor{red}{\$topSentence}.

\item Let the states be individual letters instead of individual words.
Be sure to include a state for the spaces between words.
% We will explore this particular state space choice more in Volume 3 with hidden Markov models.

\item Construct the state space so that the next state depends on both the current and previous states.
This kind of Markov chain is called a \emph{Markov chain of order 2}.
This way, every set of three consecutive words in a randomly generated sentence should be part of the training set, as opposed to only every consecutive pair of words coming from the set.

\item Instead of generating random sentences from a single source, simulate a random conversation between $n$ people.
Construct a Markov chain $M_i,$ for each person, $i=1,\ \ldots,\ n$, then create a Markov chain $C$ describing the conversation transitions from person to person; in other words, the states of $C$ are the $M_i$.
To create the conversation, generate a random sentence from the first person using $M_1$.
Then use $C$ to determine the next speaker, generate a random sentence using their Markov chain, and so on.
\end{itemize}

\subsection*{Natural Language Processing Tools} % -----------------------------

The Markov model of Problems \ref{prob:markov-random-sentences-init} and \ref{prob:markov-random-sentences-babble} is a \emph{natural language processing} application.
Python's \li{nltk} module (natural language toolkit) has many tools for parsing and analyzing text for these kinds of problems \cite{bird2004nltk}.
For example, \li{nltk.sent_tokenize()} reads a single string and splits it up into sentences.
This could be useful, for example, in making the \li{SentenceGenerator} class compatible with files that do not have one sentence per line.

\begin{lstlisting}
>>> from nltk import sent_tokenize
>>> with open("yoda.txt", 'r') as yoda:
...     sentences = sent_tokenize(yoda.read())
...
>>> print(sentences)
<<['Away with your weapon!',
 'I mean you no harm.',
 'I am wondering - why are you here?',
 ...>>
\end{lstlisting}

The \li{nltk} module is \textbf{not} part of the Python standard library.
For instructions on downloading, installing, and using \li{nltk}, visit \url{http://www.nltk.org/}.
