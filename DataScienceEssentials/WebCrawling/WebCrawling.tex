\lab{Web Crawling}{Web Crawling}

\objective{Gathering data from the internet often requires information from several web pages.
In this lab, we present two methods for crawling through multiple web pages without violating copyright laws or straining the load on a server.
We also demonstrate how to scrape data from asynchronously loaded web pages and how to interact programmatically with web pages when needed.}


\begin{comment}

\begin{info}
For grading purposes, before returning a value in each function of this lab, write the value being returned to a \li{pickle} file.
Name the file \li{ans1} if it contains the returned value of Problem 1, \li{ans2} for Problem 2, and so on, so that you have five pickle files saved.
In order to create a pickle file, use the following code:
\begin{lstlisting}
>>> import pickle
>>>
>>> # Write the answer of Problem 1, "value", to a pickle file
>>> with open("ans1", "wb") as fp:
>>>     pickle.dump(value, fp)
\end{lstlisting}
\end{info}
\end{comment}

\section*{Scraping Etiquette} % ===============================================

% TODO: this could be in the intro of something.
% Web scraping has utility in countless research and analysis scenarios where gathering and structuring data from the web is a necessity.
% There may be no suitable, accessible web API, and doing the job manually is often impractical and slow, sometimes impossible.

There are two main ways that web scraping can be problematic for a website owner.
\begin{enumerate}
\item The scraper doesn't respect the website's terms and conditions or gathers private or proprietary data.
\item The scraper imposes too much extra server load by making requests too often or in quick succession.
\end{enumerate}
These are extremely important considerations in any web scraping program.
Scraping copyrighted information without the consent of the copyright owner can have severe legal consequences.
Many websites, in their terms and conditions, prohibit scraping parts or all of the site.
Websites that do allow scraping usually have a file called \texttt{robots.txt} (for example, \url{www.google.com/robots.txt}) that specifies which parts of the website are off-limits, and how often requests can be made according to the \emph{robots exclusion standard}.%
\footnote{See \href{http://www.robotstxt.org/orig.html}{\texttt{www.robotstxt.org/orig.html}} and \href{https://en.wikipedia.org/wiki/Robots_exclusion_standard}{\texttt{en.wikipedia.org/wiki/Robots\_exclusion\_standard}}.}


\begin{warn} % Don't scrape illegally.
Be careful and considerate when doing any sort of scraping, and take care when writing and testing code to avoid unintended behavior.
It is up to the programmer to create a scraper that respects the rules found in the terms and conditions and in \texttt{robots.txt}.
Make sure to scrape websites legally.
%\footnote{Python provides a parsing library called \texttt{urllib.robotparser} for reading \texttt{robot.txt} files.
%For more information, see \url{https://docs.python.org/3/library/urllib.robotparser.html}.
%}
\end{warn}
\begin{comment} %This is almost word-for-word in the previous lab
The standard way to get the HTML source code of a website using Python is via the \li{requests} library.\footnote{Though \texttt{requests} is not part of the standard library, it is recognized as a standard tool in the data science community. See \url{http://docs.python-requests.org/}.}
Calling \li{requests.get()} sends an HTTP GET request to a specified website; the result is an object with a response code that indicates whether or not the request was successful and if so, access to the website source code.

\begin{lstlisting}
>>> import requests

# Make a request and check the result. A status code of 200 is good.
>>> response = requests.get("http://www.example.com")
>>> print(response.status_code, response.ok, response.reason)
<<200 True OK>>

# The HTML of the website is stored in the 'text' attribute.
>>> print(response.text)
<<<!doctype html>
<html>
<head>
    <title>Example Domain</title>

    <meta charset="utf-8" />
    <meta http-equiv="Content-type" content="text/html; charset=utf-8" />>>
# ...
\end{lstlisting}
\end{comment}

Recall that consecutive requests without pauses can strain a website's server and provoke retaliation.
Most servers are designed to identify such scrapers, block their access, and sometimes even blacklist the user.
This is especially common in smaller websites that aren't built to handle enormous amounts of traffic.
To briefly pause the program between requests, use \li{time.sleep()}.
\begin{lstlisting}
>>> import time
>>> time.sleep(3)      # Pause execution for 3 seconds.
\end{lstlisting}

The amount of necessary wait time depends on the website.
Sometimes, \texttt{robots.txt} contains a \li{Crawl-delay} directive which gives a number of seconds to wait between successive requests.
If this doesn't exist, pausing for a half-second to a second between requests is typically sufficient.
An email to the site's webmaster is always the safest approach and may be necessary for large scraping operations.

Python provides a parsing library called \li{urllib.robotparser} for reading \li{robot.txt} files.
Below is an example of using this library to check where robots are allowed on arxiv.org.
A website's \texttt{robots.txt} file will often include different instructions for specific crawlers.
These crawlers are identified by a \li{User-agent} string.
For example, Google's webcrawler, \li{User-agent} Googlebot, may be directed to index only the pages the website wants to have listed on a Google search.
We will use the default \li{User-agent}, \li{"*"}.

\begin{lstlisting}
>>> from urllib import robotparser
>>> rp = robotparser.RobotFileParser()
# Set the URL for the robots.txt file. Note that the URL contains `robots.txt'
>>> rp.set_url("https://arxiv.org/robots.txt")
>>> rp.read()
# Request the crawl-delay time for the default User-agent
>>> rp.crawl_delay("*")
15
# Check if User-agent "*" can access the page
>>> rp.can_fetch("*", "https://arxiv.org/archive/math/")
True
>>> rp.can_fetch("*", "https://arxiv.org/IgnoreMe/")
False
\end{lstlisting}

\begin{problem} %Problem 1: robots.txt
Write a program that accepts a web address defaulting to the site \url{http://example.webscraping.com} and a list of pages defaulting to \li{["/", "/trap", "/places/default/search"]}.
For each page, check if the \texttt{robots.txt} file permits access.
Return a list of boolean values corresponding to each page.
Also return the crawl delay time.
%Write out to your pickle file a tuple containing a list of boolean values corresponding to each page and the crawl delay time.
\label{problem:robots-file}
\end{problem}

\section*{Crawling Through Multiple Pages} % ==================================

While web \emph{scraping} refers to the actual gathering of web-based data, web \emph{crawling} refers to the navigation of a program between webpages.
Web crawling allows a program to gather related data from multiple web pages and websites.

Consider \href{http://books.toscrape.com}{\texttt{books.toscrape.com}}, a site to practice web scraping that mimics a bookstore.
The page \url{http://books.toscrape.com/catalogue/category/books/mystery_3/index.html} lists mystery books with overall ratings and review.
More mystery books can be accessed by clicking on the \texttt{next} link.
The following example demonstrates how to navigate between webpages to collect all of the mystery book titles.

\begin{lstlisting}
def scrape_books(start_page = "index.html"):
    """ Crawl through http://books.toscrape.com and extract mystery titles"""

    # Initialize variables, including a regex for finding the 'next' link.
    base_url="http://books.toscrape.com/catalogue/category/books/mystery_3/"
    titles = []
    page = base_url + start_page                # Complete page URL.
    next_page_finder = re.compile(r"next")      # We need this button.

    current = None

    for _ in range(4):
        while current == None:  # Try downloading until it works.
            # Download the page source and PAUSE before continuing.
            page_source = requests.get(page).text
            time.sleep(1)       # PAUSE before continuing.
            soup = BeautifulSoup(page_source, "html.parser")
            current = soup.find_all(class_="product_pod")

        # Navigate to the correct tag and extract title
        for book in current:
            titles.append(book.h3.a["title"])

        # Find the URL for the page with the next data.
        if "page-4" not in page:
            new_page = soup.find(string=next_page_finder).parent["href"]
            page = base_url + new_page      # New complete page URL.
            current = None
    return titles

\end{lstlisting}

In this example, the \li{for} loop cycles through the pages of books, and the \li{while} loop ensures that each website page loads properly: if the downloaded \li{page_source} doesn't have a tag whose class is \li{product_pod}, the request is sent again.
After recording all of the titles, the function locates the link to the next page.
This link is stored in the HTML as a relative website path (\texttt{page-2.html}); the complete URL to the next day's page is the concatenation of the base URL \url{http://books.toscrape.com/catalogue/category/books/mystery_3/} with this relative link.

\begin{problem} % Collect the average price of fiction books.
Modify \li{scrape_books()} so that it gathers the price for each fiction book and returns the mean price, in $\pounds$, of a fiction book.

\end{problem}
%=============================
\begin{comment}
An alternative approach that is often useful is to first identify the links to relevant pages, then scrape each of these page in succession.
For example, the Federal Reserve releases quarterly data on large banks in the United States at \url{http://www.federalreserve.gov/releases/lbr}.
The following function extracts the four measurements of total consolidated assets for JPMorgan Chase during 2004.

\begin{lstlisting}
def bank_data():
    """Crawl through the Federal Reserve site and extract bank data."""
    # Compile regular expressions for finding certain tags.
    link_finder = re.<<compile>>(r"2004$")
    chase_bank_finder = re.<<compile>>(r"^JPMORGAN CHASE BK")

    # Get the base page and find the URLs to all other relevant pages.
    base_url="https://www.federalreserve.gov/releases/lbr/"
    base_page_source = requests.get(base_url).text
    base_soup = BeautifulSoup(base_page_source, "html.parser")
    link_tags = base_soup.find_all(name='a', href=True, string=link_finder)
    pages = [base_url + tag.attrs["href"] for tag in link_tags]

    # Crawl through the individual pages and record the data.
    chase_assets = []
    for page in pages:
        time.sleep(1)               # PAUSE, then request the page.
        soup = BeautifulSoup(requests.get(page).text, "html.parser")

        # Find the tag corresponding to Chase Banks's consolidated assets.
        temp_tag = soup.find(name="td", string=chase_bank_finder)
        for _ in range(10):
            temp_tag = temp_tag.next_sibling
        # Extract the data, removing commas.
        chase_assets.append(int(temp_tag.string.replace(',', '')))

    return chase_assets
\end{lstlisting}

% Just as in the previous example, the base URL \url{http://www.federalreserve.gov/release/lbr/} is concatenated with the extension URLs found on the base page.

\begin{problem} % Crawl through bank data.
Modify \li{bank_data()} so that it extracts the total consolidated assets (``Consol Assets'') for JPMorgan Chase, Bank of America, and Wells Fargo recorded each December from 2004 to the present.
Return a list of lists where each list contains the assets of each bank.
\end{problem}

\begin{problem} % Crawl through sports data.
The Basketball Reference website at \url{https://www.basketball-reference.com} contains data on NBA athletes, including which player led different categories for each season.
For the past ten seasons, identify which player had the most season points and find how many points they scored during that season.
Return a list of triples consisting of the season, the player, and the points scored, ("season year", "player name", points scored).
\end{problem}
\end{comment}
%===============================
\begin{comment} % Old problem options.
\begin{enumerate}
\item Load \url{http://www.google.com/finance} into BeautifulSoup.
Towards the bottom of the web page, there is a Sector summary.
Go through each sector and locate the top five Gainers.
In a SQL table, store the Name, abbreviation, \% Change, and Mkt Cap of the top Gainer for each Sector.

\item Load  into BeautifulSoup.
Go through the top five offensive leaders.
In a SQL table, store the name, career games played, career mins per game, career points per game, and career FG\% for each player.

\item Load \url{http://www.foxsports.com/soccer/united-states-women-team-stats} into BeautifulSoup.
Go though each player on the World Cup US women's team.
In a SQL table, store the name, hometown, position, and \# of games played in the World Cup.
\end{enumerate}
\end{comment}

\section*{Asynchronously Loaded Content and User Interaction} % ===============

Web crawling with the methods presented in the previous section fails under a few circumstances.
First, many webpages use \emph{JavaScript}, the standard client-side scripting language for the web, to load portions of their content  \emph{asynchronously}.
This means that at least some of the content isn't initially accessible through the page's source code (for example, if you have to scroll down to load more results).
Second, some pages require user interaction, such as clicking buttons which aren't links (\li{<a>} tags which contain a URL that can be loaded) or entering text into form fields (like search bars).

\begin{comment} % Old example; this site is blocked by BYU OIT for gambling.
Navigate to the website \url{http://www.simplesoccerstats.com/stats/teamstats.php?lge=14&type=goals&season=0}.
Notice that there is a row for the Chicago team displayed in the browser.
Open up the page source,
Hit \li{ctrl+f} and search for ``Chicago''.
Notice that it isn't there.
The following code will produce a similar result:

\begin{lstlisting}
>>> soccer_url = 'http://www.simplesoccerstats.com/stats/teamstats.php?lge=14&type=goals&season=0'
>>> soccer_content = requests.get(soccer_url).text
>>> soccer_soup = BeautifulSoup(soccer_content)

>>> print(soccer_soup.find(string='Chicago'))
<<None>>
\end{lstlisting}
Still nothing.
This means the actual table of information is loaded asynchronously.
\end{comment}

The \emph{Selenium} framework provides a solution to both of these problems.
Originally developed for writing unit tests for web applications, Selenium allows a program to open a web browser and interact with it in the same way that a human user would, including clicking and typing.
It also has BeautifulSoup-esque tools for searching the HTML source of the current page.

% This solves the issue of scraping asynchronously loaded content and content that is available only through direct user interaction because the browser can execute the necessary JavaScript and interact with the page in all the ways a user does.

\begin{info} % Get the driver file.
Selenium requires an executable \emph{driver} file for each kind of browser.
The following examples use Google Chrome, but Selenium supports Firefox, Internet Explorer, Safari, Opera, and PhantomJS (a special browser without a user interface).
See \url{https://seleniumhq.github.io/selenium/docs/api/py} or \url{http://selenium-python.readthedocs.io/installation.html} for installation instructions and driver download instructions.

If you are using WSL and Windows, type these this into your Ubuntu terminal:
\begin{lstlisting}
wget dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
sudo apt install -y ./google-chrome-stable_current_amd64.deb
\end{lstlisting}

If your program still can't find the driver after you've downloaded it, add the argument \li{executable_path  = "path/to/driver/file"} when you call \li{webdriver}.
If this doesn't work, you may need to add the location to your system PATH.

On a Mac, open the file \li{/etc/path} and add the new location.

On Linux, add \li{export PATH="path/to/driver/file:\$PATH"} to the file \li{/.bashrc} .

On non-WSL Windows, follow a tutorial such as this one: \url{https://www.architectryan.com/2018/03/17/add-to-the-path-on-windows-10/}.

\end{info}

To use Selenium, start up a browser using one of the drivers in \li{selenium.webdriver}.
The browser has a \li{get()} method for going to different web pages, a \li{page_source} attribute containing the HTML source of the current page, and a \li{close()} method to exit the browser.

\begin{lstlisting}
>>> from selenium import webdriver

# Start up a browser and go to example.com.
>>> browser = webdriver.Chrome()
>>> browser.get("https://www.example.com")

# Feed the HTML source code for the page into BeautifulSoup for processing.
>>> soup = BeautifulSoup(browser.page_source, "html.parser")
>>> print(soup.prettify())
<<<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <title>
   Example Domain
  </title>
  <meta charset="utf-8"/>
  <meta content="text/html; charset=utf-8" http-equiv="Content-type"/>>>
# ...

>>> browser.close()                 # Close the browser.
\end{lstlisting}


% The \li{webdriver} is the object through which commands are given to the browser, while \li{Keys} simulates pressing keyboard keys like RETURN.
Selenium can deliver the HTML page source to BeautifulSoup, but it also has its own tools for finding tags in the HTML.

\begin{table}[H]
\centering
\begin{tabular}{l|l}
    Method & Returns \\ \hline
    % \li{get()} & Go to a specified web page. \\
    % \li{close()} & Close the browser. \\
    \li{find_element('css', 'tag\_name\_here')} & The first tag with the given name \\
    \li{find_element('name', 'tag\_name\_here')} & The first tag with the specified \li{<<name>>} attribute \\
    \li{find_element('class name', 'tag\_name\_here')} & The first tag with the given \li{<<class>>} attribute \\
    \li{find_element('id', 'tag\_name\_here')} & The first tag with the given \li{<<id>>} attribute \\
    \li{find_element('link text', 'tag\_name\_here')} & The first tag with a matching \li{<<href>>} attribute \\
    \li{find_element('partial link text',  'tag\_name\_here')} & The first tag with a partially matching \li{<<href>>} attribute \\
\end{tabular}
\caption{The method of the \li{selenium.webdriver.Chrome} class.}
\label{table:selenium-chrome-driver}
\end{table}
Note other documentation uses a format like \li{find_element(By.TAG_NAME, 'tag\_name\_here')}. To use that format, make sure to use an import statement like  \li{from selenium.webdriver.common.by import By}.

The \li{find_element()} method returns a single object representing a \emph{web element} (of type \li{selenium.webdriver.remote.webelement.WebElement}), much like a BeautifulSoup tag (of type \li{bs4.element.Tag}).
If no such element can be found, a Selenium \li{NoSuchElementException} is raised.
If you want to find \emph{all} the matching object, each webdriver also has a \li{find_elements()} method (element\textbf{s}, plural) that return a list of all matching elements, or an empty list if there are no matches.

Web element objects have methods that allow the program to interact with them: \li{click()} sends a click, \li{send_keys()} enters in text, and \li{clear()} deletes existing text.
This functionality makes it possible for Selenium to interact with a website in the same way that a human would.

For example, the following code opens up \li{https://www.google.com}, types ``Python Selenium Docs'' into the search bar, and hits enter.

\begin{lstlisting}
>>> from selenium.webdriver.common.keys import Keys
>>> from selenium.common.exceptions import NoSuchElementException

>>> browser = webdriver.Chrome()
>>> try:
...     browser.get("https://www.google.com")
...     try:
...         # Get the search bar, type in some text, and press Enter.
...         search_bar = browser.find_element('name', 'q')
...         search_bar.clear()                  # Clear any pre-set text.
...         search_bar.send_keys("Python Selenium Docs")
...         search_bar.send_keys(Keys.RETURN)   # Press Enter.
...     except NoSuchElementException:
...         print("Could not find the search bar!")
...         raise
... finally:
...     browser.close()
...
\end{lstlisting}


\begin{comment} % Continuation of old section, removed since BYU blocks stuff.
To illustrate that using Selenium solves the problem of asynchronously-loaded content, use it to load the soccer statistics page from the previous example.
\begin{lstlisting}
>>> browser = webdriver.Chrome()
>>> soccer_url = "http://www.simplesoccerstats.com/stats/teamstats.php?lge=14&type=goals&season=0"
>>> browser.get(soccer_url)
>>> soccer_soup = BeautifulSoup(browser.page_source)
>>> browser.quit() # Closes the web browser.
>>> print(soccer_soup.find(string='Chicago').parent)
<<<td>Chicago</td>>>
\end{lstlisting}
Notice that there is now a tag containing \li{"Chicago"}.
\end{comment}


\begin{problem} %Problem 3

The Internet Archive Wayback Machine archives sites so users can find the history of websites they visit and IMDB contains a variety of information on movies. Information on the top 10 box office movies of the weekend of February 21 - 23, 2020 can be found at \url{https://web.archive.org/web/20200226124504/https://www.imdb.com/chart/boxoffice/?ref_=nv_ch_cht}.

Using BeaufiulSoup, Selenium, or both, return a list, with each title on a new row, of the top 10 movies of the week and order the list according to the total grossing of the movies, from most money to the least. Break ties using the weekend gross, from most money to the least.
\end{problem}


\section*{Using CSS Selectors}%=====================

In addition to the methods listed in Table \ref{table:selenium-chrome-driver}, you can also use CSS or XPath selectors to interact more precisely with the page source. Refer to the CSS symbols for use with Selenium Table from the WebScraping lab or \url{https://www.w3schools.com/cssref/css_selectors.php} for a review of CSS syntax.

The following code searches Google for ``Python Selenium Docs'' and then clicks on the second result.

\begin{lstlisting}
#As before, go to Google and type in the search bar,
# but this time we use CSS selectors

>>> from selenium.webdriver.common.keys import Keys
>>> from selenium.common.exceptions import NoSuchElementException
>>> from selenium.webdriver.common.by import By

>>> browser = webdriver.Chrome()
>>> try:
...		browser.get("https://google.com")
...		try:
...			search_bar = browser.find_element(By.CSS_SELECTOR,
...					 "input[name='q']")
...			search_bar.clear()
...			search_bar.send_keys("Python Selenium Docs")
...			search_bar.send_keys(Keys.RETURN)
...			try:
...				# Wait a second, then get the second search result
...				time.sleep(1)
...				# "+ div" returns the element's  next sibling with a "div" tag
...				second_result = browser.find_element(By.CSS_SELECTOR,
...						 "div[class='g'] + div")
...				try:
...					# Get the link, which is a child of second_result
...					link = second_result.find_element(By.CSS_SELECTOR, 
...						"div[class='r']")
...					link.click()
...					time.sleep(1)
...				#Remember to handle exceptions
...				except NoSuchElementException:
...					print("Could not find link")
...			except NoSuchElementException:
...				print("Could not find second result")
...		except NoSuchElementException:
...			print("Could not find the search bar")
...	finally:
...		browser.close() #browser.quit() would also work

\end{lstlisting}

In the above code, we could have used \li{find_element("class name", "div[class='r']")}, but when you need more precision than that, CSS selectors can be very useful.
Remember that to view specific HTML associated with an object in Chrome or Firefox, you can right click on the object and click ``Inspect.''
For Safari, you need to first enable ``Show Develop menu'' in ``Preferences'' under ``Advanced.''
Keep in mind that you can also search through the source code (ctrl+f or cmd+f) to make sure you're using a unique identifier.

\begin{comment} %arXiv.org's robots.txt file prohibits web crawlers to use the search page
\begin{problem}
The arXiv (pronounced ``archive'') is an online repository of scientific publications, hosted by Cornell University.
Write a function that accepts a string to serve as a search query defaulting to \li{linkedin}.
Use Selenium to enter the query into the search bar of \url{https://arxiv.org} and press Enter.
The resulting page has up to 50 links to the PDFs of technical papers that match the query.
Gather these URLs, then continue to the next page (if there are more results) and continue gathering links until obtaining at most 150 URLs.
Return the list of URLs.
\end{problem}
\end{comment}
\begin{info}
Using Selenium to access a page's source code is typically much safer, though slower, than using \li{requests.get()}, since Selenium waits for each web page to load before proceeding.
For instance, some websites are somewhat defensive about scrapers, but Selenium can sometimes make it possible to gather info without offending the administrators.
\end{info}

\begin{comment} % old problem
\begin{problem}
The NBA has live statistics \url{http://stats.nba.com/}.
Use Selenium to return a list of the \li{a} tags containing each of the 30 NBA teams.
Use the \li{find_all()} method in conjunction with whatever unique identifiers get you the correct tags.
\\(Hint: class and tag name are a good start). % What even is this?

\begin{itemize}
\item The column titles are Name, HW\%, AW\%, where Name is each team name, HW\% is the Home Win \%, and AW\% is the Away Win \%.
\item Each row represents a different basketball team, with its home and away win percentages.
\end{itemize}
Hint: You will need to use Selenium to access each teams website using the links from the tags found in problem \ref{prob:scraping-bball}.
If the websites do not load properly, consider a \li{try-except} clause like the one suggested previously.
\end{problem}
\end{comment}

\begin{problem} %Problem 4
\emph{Project Euler} (\url{https://projecteuler.net}) is a collection of mathematical computing problems.
Each problem is listed with an ID, a description/title, and the number of users that have solved the problem.

Using Selenium, BeautifulSoup, or both, record the number of people who have solved each of the 700+ problems in the archive at \url{https://projecteuler.net/archives}.
Plot the number of people who have solved each problem against the problem IDs, using a log scale for the y-axis.
Display the scatter plot, then state the IDs of which problems have been solved most and least number of times.
%\\(Hint: start by identifying the URLs to each archive page.)
\end{problem}



\begin{problem} %Dynamic scraping problem
The website \url{http://watir.com/examples/simple_form.html} contains an example form you can fill out.
Using Selenium fill out the form for this example person:
\begin{lstlisting}
First name: Elizabeth
Last name: Bennet
Email: elizabeth.bennet@pemberleymail.com
Interests: Dancing, Books
Want to recieve newsletter: Yes
\end{lstlisting}
Remember to use a crawl delay of at least 5 seconds so you don't send your requests too fast.
Print out the Response text that filling out the form provides.
\end{problem}

%todo this problem works only because the site is an example site. Real sites use shadow DOMs. A shadow DOM provides a way to scope CSS styles to a specific DOM subtree and isolate that subtree from the rest of the document, and makes web crawling hard. archive.org is a great web crawling website, but it's hard to crawl.

%here is some example code from an example website. 
\begin{comment}
#Example code to put in the pdf from https://www.youtube.com/watch?v=OhGY_ZNBsu0
def get_text_from_shadow_root():
    url = 'http://watir.com/examples/shadow_dom.html'
    browser = webdriver.Chrome()
    try:
        #open website
        browser.get(url)
        time.sleep(10)

        #create locator for where the shadow root starts
        root_of_shadow = browser.find_element(By.CSS_SELECTOR, '[id="shadow_host"]').shadow_root

        #find an element under the shadow root
        shadow_text = root_of_shadow.find_element(By.CSS_SELECTOR, '[id="shadow_content"]').text
         
    finally:
        browser.close()
        
    return shadow_text

print(get_text_from_shadow_root())

def get_text_from_nested_shadow_root():
    url = 'http://watir.com/examples/shadow_dom.html'
    driver = webdriver.Chrome()
    try:
        #open website
        driver.get(url)

        #wait time
        time.sleep(10)

        #create locator for where the shadow root starts
        shadow_root_1 = driver.find_element(By.CSS_SELECTOR, '[id="shadow_host"]').shadow_root
        shadow_root_2 = shadow_root_1.find_element(By.CSS_SELECTOR, '[id="nested_shadow_host"]').shadow_root
        nested_shadow_text = shadow_root_2.find_element(By.CSS_SELECTOR, '[id="nested_shadow_content"]').text
    
    finally:
        driver.close()
    return nested_shadow_text

print(get_text_from_nested_shadow_root())

\end{comment}
