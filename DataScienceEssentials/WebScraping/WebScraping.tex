\lab{Web Scraping}{Web Scraping}
\labdependencies{Pandas1,Pandas2}
\objective{Web Scraping is the process of gathering data from websites on the internet.
Since almost everything rendered by an internet browser as a web page uses HTML, the first step in web scraping is being able to extract information from HTML.
In this lab, we introduce the requests library for scraping web pages, BeautifulSoup: Python's canonical tool for efficiently and cleanly navigating and parsing HTML,
and how to use Pandas to extract data from HTML tables.}

\section*{HTTP and Requests} % =============================================================

HTTP stands for Hypertext Transfer Protocol, which is an application layer networking protocol.
It is a higher level protocol than TCP, which we used to build a server in the Web Technologies lab, but uses TCP protocols to manage connections and provide network capabilities.
The HTTP protocol is centered around a request and response paradigm, in which a client makes a request to a server and the server replies with a response.
There are several methods, or \emph{requests}, defined for HTTP servers, the three most common of which are GET, POST, and PUT.
A GET request asks for information from the server, a POST request modifies the state of the server, and a PUT request adds new pieces of data to the server.

The standard way to get the source code of a website using Python is via the \li{requests} library.\footnote{Though \texttt{requests} is not part of the standard library, it is recognized as a standard tool in the data science community. See \url{http://docs.python-requests.org/}.}
Calling \li{requests.get()} sends an HTTP GET request to a specified website.
The website returns a response code, which indicates whether or not the request was received, understood, and accepted.
If the response code is good, typically \li{200}\footnote{See \url{https://en.wikipedia.org/wiki/List_of_HTTP_status_codes} for explanation of specific response codes.}, then the response will also include the website source code as an HTML file.

\begin{lstlisting}
>>> import requests

# Make a request and check the result. A status code of 200 is good.
>>> response = requests.get("http://www.byu.edu")
>>> print(response.status_code, response.ok, response.reason)
<<200 True OK>>



# The HTML of the website is stored in the "text" attribute.
>>> print(response.text)
<<<!DOCTYPE html>
<html lang="en" dir="ltr" prefix="content: http://purl.org/rss/1.0/modules/content/  dc: http://purl.org/dc/terms/  foaf: http://xmlns.com/foaf/0.1/  og: http://ogp.me/ns#  rdfs: http://www.w3.org/2000/01/rdf-schema#  schema: http://schema.org/  sioc: http://rdfs.org/sioc/ns#  sioct: http://rdfs.org/sioc/types#  skos: http://www.w3.org/2004/02/skos/core#  xsd: http://www.w3.org/2001/XMLSchema# " class=" ">

  <head>
    <meta charset="utf-8" />>>
# ...
\end{lstlisting}

\begin{table}[H]
\centering
\begin{tabular}{c|l}
Attribute & Description \\ \hline
\li{text} & Content of the response, in unicode \\
\li{content} & Content of the response, in bytes \\
\li{encoding} & Encoding used to decode response.content \\
\li{raw} & Raw socket response, which allows applications to send and obtain packets of information  \\
\li{status\_code} & Numeric code that shows how the action was successful \\
\li{ok} &Boolean that is \li{True} if the  \li{status\_code} is less than 400, or \li{False} if not \\
\li{reason} &Text corresponding to the status code \\
\end{tabular}
\caption{Different content attributes of the request class.}
\label{table:request-class-attributes}
\end{table}

Note that some websites aren't built to handle large amounts of traffic or many repeated requests.
Most are built to identify web scrapers or crawlers that initiate many consecutive GET requests without pauses, and retaliate or block them.
When web scraping, always make sure to store the data that you receive in a file and include error checks to prevent retrieving the same data unnecessarily.
We won't spend much time on that in this lab, but it's especially important in larger applications.

\begin{problem}
Use the \li{requests} library to get the HTML source for the website \url{http://www.example.com}.
Save the source as a file called \li{"example.html"}.
If the file already exists, make sure not to scrape the website or overwrite the file.
You will use this file later in the lab.

Hint: When writing responses to file you need to use the use the \li{open()} function with the appropriate file write mode ((either \li{mode='w'} for plain write mode, or \li{mode="wb"} for binary write mode). Python interpreters will write the example file the same way whether you use \li{response.text} or \li{response.content}.

\label{prob:get-example.com}
\end{problem}

\begin{warn} % Don't scrape illegally.
Scraping copyrighted information without the consent of the copyright owner can have severe legal consequences.
Many websites, in their terms and conditions, prohibit scraping parts or all of the site.
Websites that do allow scraping usually have a file called \texttt{robots.txt} (for example, \url{www.google.com/robots.txt}) that specifies which parts of the website are off-limits and how often requests can be made according to the \emph{robots exclusion standard}.%
\footnote{See \href{http://www.robotstxt.org/orig.html}{\texttt{www.robotstxt.org/orig.html}} and \href{https://en.wikipedia.org/wiki/Robots_exclusion_standard}{\texttt{en.wikipedia.org/wiki/Robots\_exclusion\_standard}}.}

Be careful and considerate when doing any sort of scraping, and take care when writing and testing code to avoid unintended behavior.
It is up to the programmer to create a scraper that respects the rules found in the terms and conditions and in \texttt{robots.txt}.%
\footnote{Python provides a parsing library called \texttt{urllib.robotparser} for reading \texttt{robot.txt} files.
For more information, see \url{https://docs.python.org/3/library/urllib.robotparser.html}.
}

We will cover this more in the next lab.
\end{warn}

\section*{HTML} % =============================================================

\emph{Hyper Text Markup Language}, or \emph{HTML}, is the standard \emph{markup language}---a language designed for the processing, definition, and presentation of text---for creating webpages.
It structures a document using pairs of \emph{tags} that surround and define content.
Opening tags have a tag name surrounded by angle brackets (\li{<tag-name>}).
The companion closing tag looks the same, but with a forward slash before the tag name (\li{</tag-name>}).
A list of all current HTML tags can be found at \url{http://htmldog.com/reference/htmltags}.

Most tags can be combined with \emph{attributes} to include more data about the content, help identify individual tags, and make navigating the document much simpler.
In the following example, the \li{<a>} tag has \lstinline[language=HTML]{id} and \li{href} attributes.

\begin{lstlisting}[language=HTML]
<html>                                  <!-- Opening tags -->
   <body>
       <p>
           Click <a id='info' href='http://www.example.com'>here</a>
           for more information.
       </p>                             <!-- Closing tags -->
   </body>
</html>
\end{lstlisting}

In HTML, \li{href} stands for \emph{hypertext reference}, a link to another website.
Thus the above example would be rendered by a browser as a single line of text, with \li{here} being a clickable link to \url{http://www.example.com}:

\begin{center}
Click \href{http://www.example.com}{here} for more information.
\end{center}

Unlike Python, HTML does not enforce indentation (or any whitespace rules), though indentation generally makes HTML more readable.
The previous example can be written in a single line.

\begin{lstlisting}[language=HTML]
<html><body><p>Click <a id='info' href='http://www.example.com/info'>here</a>
  for more information.</p></body></html>
\end{lstlisting}

Special tags, which don't contain any text or other tags, are written without a closing tag and in a single pair of brackets.
A forward slash is included between the name and the closing bracket.
Examples of these include \li{<hr/>}, which describes a horizontal line, and \li{<img/>}, the tag for representing an image.
%Old problem
\begin{comment}
\begin{problem} % Use View Page Source to see HTML source of a simple website.
Using the output from Problem \ref{prob:get-example.com}, examine the HTML source code for \url{http://www.example.com}.
What tags are used?
What is the value of the \li{<<type>>} attribute associated with the \li{style} tag?

Write a function that returns a set of the names of all tags the website uses and the value of the \li{<<type>>} attribute of the \li{style} tag (as a string).
\\(Hint: there are ten unique tag names.)
\label{prob:look-at-example.com}
\end{problem}
\end{comment}

\begin{info}
You can open .html files using a text editor or any web browser.
In a browser, you can inspect the source code associated with specific elements.
Right click the element and select \li{Inspect}.
If you are using Safari, you may first need to enable ``Show Develop menu'' in ``Preferences'' under the ``Advanced'' tab.
\end{info}

\section*{BeautifulSoup} % ====================================================

BeautifulSoup (\li{bs4}) is a package%
\footnote{BeautifulSoup is not part of the standard library; install it with \li{pip install beautifulsoup4}.} that makes it simple to navigate and extract data from HTML documents.
See \url{http://www.crummy.com/software/BeautifulSoup/bs4/doc/index.html} for the full documentation.

The \li{bs4.BeautifulSoup} class accepts two parameters to its constructor: a string of HTML code and an HTML parser to use under the hood.
The HTML parser is technically a keyword argument, but the constructor prints a warning if one is not specified.
The standard choice for the parser is \li{"html.parser"}, which means the object uses the standard library's \li{html.parser} module as the engine behind the scenes.

\begin{info}
Depending on project demands, a parser other than \li{"html.parser"} may be useful.
A couple of other options are \li{"lxml"}, an extremely fast parser written in C, and \li{"html5lib"}, a slower parser that treats HTML in much the same way a web browser does, allowing for irregularities.
Both must be installed independently; see \url{https://www.crummy.com/software/BeautifulSoup/bs4/doc/\#installing-a-	parser} for more information.
\end{info}

\vspace{5mm}
A \li{BeautifulSoup} object represents an HTML document as a tree.
In the tree, each tag is a \emph{node} with nested tags and strings as its \emph{children}.
The \li{prettify()} method returns a string that can be printed to represent the BeautifulSoup object in a readable format that reflects the tree structure.

\begin{lstlisting}
>>> from bs4 import BeautifulSoup

>>> small_example_html = """
<html><body><p>
    Click <a id='info' href='http://www.example.com'>here</a>
    for more information.
</p></body></html>
"""

>>> small_soup = BeautifulSoup(small_example_html, "html.parser")
>>> print(small_soup.prettify())
<<<html>
  <body>
    <p>
      Click
      <a href="http://www.example.com" id="info">
         here
      </a>
      for more information.
    </p>
  </body>
</html>>>
\end{lstlisting}

Each tag in a \li{BeautifulSoup} object's HTML code is stored as a \li{bs4.element.Tag} object, with actual text stored as a \li{bs4.element.NavigableString} object.
Tags are accessible directly through the \li{BeautifulSoup} object.

\begin{lstlisting}
# Get the <p> tag (and everything inside of it).
>>> small_soup.p
<<<p>
    Click <a href="http://www.example.com" id="info">here</a>
    for more information.
</p>>>

# Get the <a> sub-tag of the <p> tag.
>>> a_tag = small_soup.p.a
>>> print(a_tag, type(a_tag), sep='\n')
<<<a href="http://www.example.com" id="info">here</a>
<class 'bs4.element.Tag'>>>

# Get just the name, attributes, and text of the <a> tag.
>>> print(a_tag.name, a_tag.attrs, a_tag.string, sep="\n")
<<a
{'id': 'info', 'href': 'http://www.example.com'}
here>>
\end{lstlisting}

\begin{table}[H]
\centering
\begin{tabular}{c|l}
Attribute & Description \\ \hline
\li{name} & The name of the tag \\
\li{attrs} & A dictionary of the attributes \\
\li{string} & The single string contained in the tag \\
\li{strings} & Generator for strings of children tags \\
\li{stripped_strings} & Generator for strings of children tags, stripping whitespace \\
\li{text} & Concatenation of strings from all children tags
\end{tabular}
\caption{Data attributes of the \li{bs4.element.Tag} class.}
\label{table:bs4-tag-attributes}
\end{table}

\begin{problem} % Find tag names using BeautifulSoup.
The \li{BeautifulSoup} class has a \li{find_all()} method that, when called with \li{True} as the only argument, returns a list of all tags in the HTML source code.

Write a function that accepts a string of HTML code as an argument.
Use BeautifulSoup to return a list of the \textbf{names} of the tags in the code.
\end{problem}

\subsection*{Navigating the Tree Structure} % ---------------------------------

Not all tags are easily accessible from a \li{BeautifulSoup} object.
Consider the following example.

\begin{lstlisting}
>>> pig_html = """
<html><head><title>Three Little Pigs</title></head>
<body>
<p class="title"><b>The Three Little Pigs</b></p>
<p class="story">Once upon a time, there were three little pigs named
<a href="http://example.com/larry" class="pig" id="link1">Larry,</a>
<a href="http://example.com/mo" class="pig" id="link2">Mo</a>, and
<a href="http://example.com/curly" class="pig" id="link3">Curly.</a>
<p>The three pigs had an odd fascination with experimental construction.</p>
<p>...</p>
</body></html>
"""

>>> pig_soup = BeautifulSoup(pig_html, "html.parser")
>>> pig_soup.p
<<<p class="title"><b>The Three Little Pigs</b></p>>>

>>> pig_soup.a
<<<a class="pig" href="http://example.com/larry" id="link1">Larry,</a>>>
\end{lstlisting}

Since the HTML in this example has several \li{<p>} and \li{<a>} tags, only the \textbf{first} tag of each name is accessible directly from \li{pig_soup}.
The other tags can be accessed by manually navigating through the HTML tree.

Every HTML tag (except for the topmost tag, which is usually \li{<html>}) has a \emph{parent} tag.
Each tag also has zero or more \emph{sibling} and \emph{children} tags or text.
Following a true tree structure, every \li{bs4.element.Tag} in a soup has multiple attributes for accessing or iterating through parent, sibling, or child tags.

\begin{table}[H]
\centering
\begin{tabular}{c|l}
Attribute & Description \\ \hline
\li{parent} & The parent tag \\
\li{parents} & Generator for the parent tags up to the top level \\
\li{next_sibling} & The tag immediately after to the current tag \\
\li{next_siblings} & Generator for sibling tags after the current tag \\
\li{previous_sibling} & The tag immediately before the current tag \\
\li{previous_siblings} & Generator for sibling tags before the current tag \\
\li{contents} & A list of the immediate children tags\\
\li{children} & Generator for immediate children tags\\
\li{descendants} & Generator for all children tags (recursively)\\
\end{tabular}
\caption{Navigation attributes of the \li{bs4.element.Tag} class.}
\label{table:bs4-tag-attributes-2}
\end{table}



\begin{lstlisting}
# Start at the first <a> tag in the soup.
>>> a_tag = pig_soup.a
>>> a_tag
<<<a class="pig" href="http://example.com/larry" id="link1">Larry,</a>>>

# Get the names of all of <a>'s parent tags, traveling up to the top.
# The name "[document]" means it is the top of the HTML code.
>>> [par.name for par in a_tag.parents]     # <a>'s parent is <p>, whose
<<['p', 'body', 'html', '[document]']>>         # parent is <body>, and so on.

# Get the next siblings of <a>.
>>> a_tag.next_sibling
<<'\n'>>                                        # The first sibling is just text.
>>> a_tag.next_sibling.next_sibling         # The second sibling is a tag.
<<<a class="pig" href="http://example.com/mo" id="link2">Mo</a>>>
\end{lstlisting}
\begin{comment} %Reducing yellow boxes. This was commented before
# Alternatively, get all siblings past <a> at once.
>>> list(a_tag.next_siblings)
<<['\n',
 <a class="pig" href="http://example.com/mo" id="link2">Mo</a>,
 ', and\n',
 <a class="pig" href="http://example.com/curly" id="link3">Curly.</a>,
 '\n',
 <p>The three pigs had an odd fascination with experimental construction.</p>,
 '\n',
 <p>...</p>,
 '\n']>>
\end{comment}

Note carefully that newline characters are considered to be children of a parent tag.
Therefore iterating through children or siblings often requires checking which entries are tags and which are just text.
In the next example, we use a tag's \li{attrs} attribute to access specific attributes within the tag (see Table \ref{table:bs4-tag-attributes}).

\begin{lstlisting}
# Get to the <p> tag that has class="story" using these commands.
>>> p_tag = pig_soup.body.p.next_sibling.next_sibling
>>> p_tag.attrs["class"]                # Make sure it's the right tag.
<<['story']>>

# Iterate through the child tags of <p> and print hrefs whenever they exist.
>>> for child in p_tag.children:
...		# Skip the children that are not bs4.element.Tag objects
...		# These don't have the attribute "attrs"
...		if hasattr(child, "attrs") and "href" in child.attrs:
...			print(child.attrs["href"])
<<http://example.com/larry
http://example.com/mo
http://example.com/curly>>
\end{lstlisting}

Note that the \li{"class"} attribute of the \li{<p>} tag is a list.
This is because the \li{"class"} attribute can take on several values at once; for example, the tag \li{<p class="story book">} is of class \li{"story"} and of class \li{"book"}.

The behavior of the \li{string} attribute of a \li{bs4.element.Tag} object depends on the structure of the corresponding HTML tag.
\begin{enumerate}
    \item If the tag has a string of text and no other child elements, then \li{string} is just that text.
    \item If the tag has exactly one child tag and the child tag has only a string of text, then the tag has the same \li{string} as its child tag.
    \item If the tag has more than one child, then \li{string} is \li{None}.
    In this case, use \li{strings} to iterate through the child strings.
    Alternatively, the \li{get_text()} method returns all text belonging to a tag and to all of its descendants.
    In other words, it returns anything inside a tag that isn't another tag.
\end{enumerate}

\begin{lstlisting}
>>> pig_soup.head
<<<head><title>Three Little Pigs</title></head>>>

# Case 1: the <title> tag's only child is a string.
>>> pig_soup.head.title.string
<<'Three Little Pigs'>>

# Case 2: The <head> tag's only child is the <title> tag.
>>> pig_soup.head.string
<<'Three Little Pigs'>>

# Case 3: the <body> tag has several children.
>>> pig_soup.body.string is None
<<True>>
>>> print(pig_soup.body.get_text().strip())
<<The Three Little Pigs
Once upon a time, there were three little pigs named
Larry,
Mo, and
Curly.
The three pigs had an odd fascination with experimental construction.
...>>
\end{lstlisting}

\begin{problem} %Problem 3
Write a function that reads a file of the same format as the file generated from Problem \ref{prob:get-example.com} and load it into BeautifulSoup.
Find the first \li{<a>} tag, and return its text along with a boolean value indicating whether or not it has a hyperlink (\li{href} attribute).
\end{problem}

\begin{comment}
\begin{problem} % TODO: give them the html file, don't download it.

% example.html
Write a function that returns the following line using three different methods.
\begin{lstlisting}
<<'More information...'>>
\end{lstlisting}
The function should accept an integer.
If the integer is 1, find the line using tag names and the \li{.string} method.
If the integer is 2, find the line by traversing through the children of the body tag with repeated calls to \li{.contents}.
If the integer is 3, find the line by using navigation between siblings and \li{.string}.
\end{problem}
\end{comment}

\subsection*{Searching for Tags} % --------------------------------------------

Navigating the HTML tree manually can be helpful for gathering data out of lists or tables, but these kinds of structures are usually buried deep in the tree.
The \li{find()} and \li{find_all()} methods of the \li{BeautifulSoup} class identify tags that have distinctive characteristics, making it much easier to jump straight to a desired location in the HTML code.
The \li{find()} method only returns the \textbf{first} tag that matches a given criteria, while \li{find_all()} returns a list of all matching tags.
Tags can be matched by name, attributes, and/or text.

\begin{lstlisting}
# Find the first <b> tag in the soup.
>>> pig_soup.find(name='b')
<<<b>The Three Little Pigs</b>>>

# Find all tags with a class attribute of "pig".
# Since "class" is a Python keyword, use "class_" as the argument.
>>> pig_soup.find_all(class_="pig")
<<[<a class="pig" href="http://example.com/larry" id="link1">Larry,</a>,
 <a class="pig" href="http://example.com/mo" id="link2">Mo</a>,
 <a class="pig" href="http://example.com/curly" id="link3">Curly.</a>]>>

# Find the first tag that matches several attributes.
>>> pig_soup.find(attrs={"class": "pig", "href": "http://example.com/mo"})
<<<a class="pig" href="http://example.com/mo" id="link2">Mo</a>>>

# Find the first tag whose text is "Mo".
>>> pig_soup.find(string="Mo")
<<'Mo'>>                                    # The result is the actual string,
>>> pig_soup.find(string="Mo").parent   # so go up one level to get the tag.
<<<a class="pig" href="http://example.com/mo" id="link2">Mo</a>>>
\end{lstlisting}

\begin{problem}
The file \texttt{san\_diego\_weather.html} contains the HTML source for an old page from Weather Underground.\footnote{See \url{http://www.wunderground.com/history/airport/KSAN/2015/1/1/DailyHistory.html?req_city=San+Diego&req_state=CA&req_statename=California&reqdb.zip=92101&reqdb.magic=1&reqdb.wmo=99999&MR=1}}
Write a function that reads the file and loads it into BeautifulSoup.

Return a list of the following tags:
\begin{enumerate}
\item The tag containing the date ``Thursday, January 1, 2015''.
\item The tags which contain the \textbf{links} ``Previous Day'' and ``Next Day''.

Hint: the tags that contain the links do not explicitly have ``Previous Day" or ``Next Day," so you cannot find the tags based on a string. Instead, open the HTML file manually and look at the attributes of the tags that contain the links. Then inspect them to see if there is a better attribute with which to conduct a search.
\item The tag which contains the number associated with the Max Actual Temperature.

Hint: Manually inspect the HTML document in both a browser and in a text editor. Look for a good attribute shared by the Actual Temperatures with which you can conduct a search. Note that the Max Actual Temperature tag will not be the first tag found. Make sure you return the correct tag.
\end{enumerate}
\end{problem}

\subsection*{Advanced Search Techniques: Regular Expressions} % ------------------------------------

Consider the problem of finding the tag that is a link to the URL \url{http://example.com/curly}.

\begin{lstlisting}
>>> pig_soup.find(href="http://example.com/curly")
<<<a class="pig" href="http://example.com/curly" id="link3">Curly.</a>>>
\end{lstlisting}

This approach works, but it requires entering in the entire URL.
To perform generalized searches, the \li{find()} and \li{find_all()} method also accept compiled regular expressions from the \li{re} module.
This way, the methods locate tags whose name, attributes, and/or string matches a pattern.

\begin{lstlisting}
>>> import re

# Find the first tag with an href attribute containing "curly".
>>> pig_soup.find(href=re.<<compile>>(r"curly"))
<<<a class="pig" href="http://example.com/curly" id="link3">Curly.</a>>

# Find the first tag with a string that starts with "Cu".
>>> pig_soup.find(string=re.<<compile>>(r"^Cu")).parent
<<<a class="pig" href="http://example.com/curly" id="link3">Curly.</a>>>

# Find all tags with text containing "Three".
>>> [tag.parent for tag in pig_soup.find_all(string=re.<<compile>>(r"Three"))]
<<[<title>Three Little Pigs</title>, <b>The Three Little Pigs</b>]>>
\end{lstlisting}

Finally, to find a tag that has a particular attribute, regardless of the actual value of the attribute, use \li{True} in place of search values.

\begin{lstlisting}
# Find all tags with an "id" attribute.
>>> pig_soup.find_all(<<id>>=True)
<<[<a class="pig" href="http://example.com/larry" id="link1">Larry,</a>,
 <a class="pig" href="http://example.com/mo" id="link2">Mo</a>,
 <a class="pig" href="http://example.com/curly" id="link3">Curly.</a>]>>

# Find the names all tags WITHOUT an "id" attribute.
>>> [tag.name for tag in pig_soup.find_all(<<id>>=False)]
<<['html', 'head', 'title', 'body', 'p', 'b', 'p', 'p', 'p']>>
\end{lstlisting}
It is important to note that using Regular Expressions to locate tags  will only find tags whose name, attributes, and/or string match exactly. If searching through an HTML file with a Regular expression to find all the tags with an  \li{"id"} attribute with labels that you specify, it will only return the tags found with those labels, not all instances of the labels. Sometimes that is very useful in simplifying Regular Expression searches.

\subsection*{Advanced Search Techniques: CSS Selectors} % ------------------------------------

\li{BeautifulSoup} also supports the use of CSS selectors.
CSS (Cascading Style Sheet) describes the style and layout of a webpage, and CSS selectors provide a useful way to navigate HTML code.
Use the method \li{soup.select()} to find all elements matching an argument.
The general format for an argument is \li{<<tag-name[attribute-name = 'attribute value']>>}.
The table below lists symbols you can use to more precisely locate various elements.

\begin{table}[H]
\centering
\begin{tabular}{c|l}
    Symbol & Meaning \\ \hline
    \li{=} & Matches an attribute value exactly \\
    \li{*=} & Partially matches an attribute value \\
    \li{\^=} & Matches the beginning of an attribute value \\
    \li{\$=} & Matches the end of an attribute value \\
    \li{+} & Next sibling of matching element \\
    \li{>} & Search an element's children \\
\end{tabular}
\caption{CSS symbols for use with Selenium}
\label{table:selenium-css-selectors} %Referred to in Web Crawling lab
\end{table}

You can do many other useful things with CSS selectors.
A helpful guide can be found at \url{https://www.w3schools.com/cssref/css_selectors.asp}.
The code below gives an example using arguments described above.

\begin{lstlisting}
# Find all <a> tags with id="link1"
>>> pig_soup.select("[id='link1']")
[<a class="pig" href="http://example.com/larry" id="link1">Larry,</a>]

# Find all tags with an href attribute containing "curly".
>>> pig_soup.select("[href*='curly']")
[<a class="pig" href="http://example.com/curly" id="link3">Curly.</a>]

# Find all <a> tags with an href attribute
>>> pig_soup.select("a[href]")
[<a class="pig" href="http://example.com/larry" id="link1">Larry,</a>,
 <a class="pig" href="http://example.com/mo" id="link2">Mo</a>,
 <a class="pig" href="http://example.com/curly" id="link3">Curly.</a>]

 # Find all <b> tags within a <p> tag with class="title"
 >>> pig_soup.select("p[class='title'] b")
 [<b>The Three Little Pigs</b>]

 # Use a comma to find elements matching one of two arguments
 >>> pig_soup.select("a[href$='mo'],[id='link3']")
[<a class="pig" href="http://example.com/mo" id="link2">Mo</a>,
 <a class="pig" href="http://example.com/curly" id="link3">Curly.</a>]
\end{lstlisting}

\begin{problem} % Bank Data Index.
The file \texttt{large\_banks\_index.html} is an index of data about large banks, as recorded by the Federal Reserve.\footnote{See \url{https://www.federalreserve.gov/releases/lbr/}.}
Write a function that reads the file and loads the source into BeautifulSoup.
Return a list of the \li{<a>} tags containing the links to bank data from September 30, 2003 to December 31, 2014, where the dates are in reverse chronological order.
\label{prob:bs-bank-index}
\end{problem}
\subsection*{Incorporating Pandas with HTML Tables} % ------------------------------------
The Pandas \texttt{pd.read\_html} method is a neat way to automatically read tables into DataFrames. It works similarly to \li{BeautifulSoup}, but is streamlined to only scrape all the tables from HTML pages.
\begin{lstlisting}
>>> import pandas as pd
#Read in "file.html"
>>> tables = pd.read_html("file.html")
# Choose the correct table that pd.read_html found as a DataFrame
>>> df = tables[0]
# The DataFrame is now like any other Pandas DataFrame that needs a bit of data cleaning
>>> df["column_1"]
0       0
1       2
2       .
3       6
4       8
       ..
15     30
16     32
17     .
18     .
19     38

\end{lstlisting}
Like any other DataFrame table, if there are non-numerical values in a column, the column type will default to strings, so make sure when sorting values by number, set the column data type to numeric with \texttt{pd.to\_numeric}.

For more help with \texttt{pd.read\_html} see \url{https://pandas.pydata.org/docs/reference/api/pandas.read_html.html}.

\begin{problem} % Actual Bank Data.
The file \texttt{large\_banks\_data.html} is one of the pages from the index in Problem \ref{prob:bs-bank-index}.\footnote{See \url{http://www.federalreserve.gov/releases/lbr/20030930/default.htm}.}
Read the specified file and load it into Pandas.

Create a single figure with two subplots:
\begin{enumerate}
    \item A sorted bar chart of the seven banks with the most domestic branches.
    \item A sorted bar chart of the seven banks with the most foreign branches.
\end{enumerate}
\end{problem}
