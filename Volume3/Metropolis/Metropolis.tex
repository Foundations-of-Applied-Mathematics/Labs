\lab{Metropolis Algorithm}{Metropolis Algorithm}
\objective{Understand the basic principles of the Metropolis algorithm and apply these ideas to the
Ising Model.}

\section*{The Metropolis Algorithm}
Sampling from a given probability distribution is an important part of many tasks throughout the sciences.
When modeling real-world problems, these distributions are often very complicated, and direct sampling methods require computing high-dimensional integrals and are thus impractical.
The Metropolis algorithm is an effective method to sample from many of these distributions.
This algorithm only requires evaluating the probability density function up to a constant of proportionality. 
In particular, the Metropolis algorithm does not require us to compute any difficult high-dimensional integrals, such as those that are found
in the denominator of Bayesian posterior distributions.

Suppose that $h : \mathbb{R}^n \rightarrow \mathbb{R}$ is the probability density function of a distribution that is difficult to evaluate (for example, a Bayesian posterior distribution), while some function $f(\boldsymbol{\theta}) = c \cdot h(\boldsymbol{\theta})$ is easy to evaluate.
The Metropolis algorithm is an MCMC sampling method which constructs a Markov chain $Y$ whose invariant distribution is exactly the distribution associated with $h$.
We can then use these samples as a sample from this distribution.

For the Metropolis algorithm, we need two ingredients: a \emph{proposal function}, and an \emph{acceptance function}.
The proposal function is used to choose a potential next state.
We denote this function as $Q(\x,\y): \mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}$.
For each $\y\in \mathbb{R}^n$, $Q(\cdot, \y)$ is the probability density function for the proposed state.
This distribution needs to be easy to sample from; typical choices are uniform or normal distributions.
For simplicity we also require this function to be \emph{symmetric}, so $Q(\x,\y)=Q(\y,\x)$.
In words, the probability density of moving from $\x$ to $\y$ is the same as moving from $\y$ to $\x$ for all $\x$ and $\y$.

The acceptance function $A : \mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}$ gives the probability that we actually transition from the current state $\y$ to the proposed state $\x$.\footnote{In the Volume 3 textbook, the proposal function for continuous distributions and the acceptance function are denoted $f_{X_{t+1}|X_t=\y}(\x)$ and $a_{\x,\y}$ respectively. In this lab we instead write them as $Q(\x,\y)$ and $A(\x,\y)$ to make it clearer that they are functions of $\x$ and $\y$.}
This function is defined by
\[
A(\x,\y) = \min\left(1, \frac{f(\x)}{f(\y)}\right).
\]
Following the proposals from $Q$ causes us to wander around the space of allowed states.
The acceptance function from $A$ modifies this wandering so that we spend more time in more likely regions.

\begin{algorithm}
\begin{algorithmic}[1]
\Procedure{Metropolis Algorithm}{}
    \State \textrm{Choose initial point } $\y_0$.
    \For{$t=1,2,\ldots$}
        \State \textrm{Draw } $\x \sim Q(\cdot, \y_{t-1})$
        \State \textrm{Draw } $a \sim \text{unif}(0,1)$
        \If{$a \leq A(\x,\y_{t-1})$}
            \State $\y_t = \x$
        \Else
            \State $\y_t = \y_{t-1}$
        \EndIf
    \EndFor
    \State \textrm{Return } $\y_1,\y_2,\y_3,\ldots$
\EndProcedure
\end{algorithmic}
\caption{Metropolis Algorithm}
\label{alg:metropolis}
\end{algorithm}
These functions form the basis for the Metropolis algorithm.
At each step, given our current state $\y_t$, we propose a new state according to the distribution $\x\sim Q(\cdot, \y_t)$. 
We then accept the proposed state with probability $A(\x, \y_t)$.
If we accept the proposal, we set $\y_{t+1}=\x$; otherwise, we set $\y_{t+1}=\y_t$.
Refer to Algorithm \ref{alg:metropolis} for a write-up of this algorithm.
Under certain conditions on $Q$, the Markov chain the samples $\y_1,\y_2,\y_3\ldots$ are from will have a unique invariant distribution with density $h$, and any initial state will converge to this distribution.

We can consider each of the samples $\y_i$ as draws from the distribution of $h$.
Most of the time we don't just want samples from the distribution, but \emph{independent} samples.
However, the samples $\y_t$ and $\y_{t+1}$ are clearly not independent.
We can get around this issue by only keeping some of the samples, for example every 10th or 100th sample.
While $\y_t$ and $\y_{t+1}$ aren't independent, $\y_t$ and $\y_{t+100}$ will be closer to being independent.

Finally, for numerical reasons, it is often wise to make calculations of the acceptance functions in log space:
\[
\log A(\x,\y) = \min(0, \log f(\x) - \log f(\y)).
\]


Let's apply the Metropolis algorithm to an example of Bayesian analysis.
Consider the exam scores in \texttt{examscores.csv}, and suppose that these scores are distributed normally with (unknown) mean $\mu$ and variance $\sigma^2$.
We wish to compute the posterior distribution for $\mu$ and $\sigma^2$.
Denote the data as $d_1,\ldots,d_N$ and assume the prior distributions
\begin{align*}
\mu &\sim \mathcal{N}(m=80,\ s^2=16)\\
\sigma^2 &\sim IG(\alpha=3,\beta=50).
\end{align*}
Note that $IG$ is the inverse gamma distribution.
In this situation, we wish to sample from the posterior distribution
\[
p(\mu,\sigma^2 \,|\,d_1,\ldots,d_N) 
= \frac{
p(\mu)p(\sigma^2)\prod_{i=1}^N \mathcal{N}(d_i \, | \, \mu, \sigma^2)
}
{
\int_{-\infty}^\infty
\int_{0}^\infty 
p(\mu')p({\sigma^2}')
\prod_{i=1}^N
\mathcal{N}(d_i \, | \, \mu', {\sigma^2}')
\,d{\sigma^2}' d\mu'
}.
\]
However, we can conveniently calculate only the numerator of this expression.
Since the denominator is simply a constant with respect to $\mu$ and $\sigma^2$, the numerator can serve as the function $f$ in the Metropolis algorithm, and the denominator can serve as the constant $c$.

We choose our proposal function to be based on a bivariate Normal distribution:
\[
Q(\x,\y) = \mathcal{N}(\x\, | \, \y, uI),
\]
i.e. normally distributed with mean $\y$ and variance $uI$
where $I$ is the $2\times 2$ identity matrix and $u>0$.
\begin{lstlisting}
def proposal(y, u):
    """Returns the proposal, i.e. a draw from Q(x,y) = N(x|y,uI)."""
    return stats.multivariate_normal.rvs(mean=y, cov=u*np.eye(len(x)))
    
def propLogDensity(x, muprior, sig2prior, scores):
    """Calculate the log of the proportional density funciton f."""
    if x[1] <= 0:
        return -np.inf
    logprob = muprior.logpdf(x[0]) + sig2prior.logpdf(x[1])
    logprob += stats.norm.logpdf(scores, loc=x[0], scale=np.sqrt(x[1])).sum()
    return logprob
    
def acceptance(x, y, muprior, sig2prior, scores):
    """
    Returns the acceptance probability of moving from y to x.
    """
    return np.exp(min(0, 
                  propLogDensity(x, muprior, sig2prior, scores) 
                - propLogDensity(y, muprior, sig2prior, scores)
    ))
\end{lstlisting}

We are now ready to code up the Metropolis algorithm using these functions.
We will keep track of the samples generated by the algorithm, along with the proportional log probabilities $\log f(\y_t)$ and the proportion of proposed samples that were accepted.

We will evaluate the quality of our results by plotting the log probabilities, the $\mu$ samples, the $\sigma^2$ samples, and kernel density estimators for the marginal posterior distributions of $\mu$ and $\sigma^2$. 
The kernel density estimators approximate the continuous distribution of the marginal distributions. 
The kernel density estimator for $\mu$ should be approximately normal, and the kernel density estimator for $\sigma^2$ should be approximately an inverse gamma.
\begin{figure}[H]
    \centering
    \includegraphics[width=.7\textwidth]{figures/logprobs.pdf}
    \caption{Log densities of the first 500 Metropolis samples.}
    \label{fig:logprobs}
\end{figure}

\begin{figure}[H]
    \begin{subfigure}[b]{.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/mu_traces.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/mu_kernel.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/sig_traces.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/sig_kernel.pdf}
    \end{subfigure}
\caption{Metropolis samples and KDEs for the marginal posterior distribution of $\mu$ (top row) and $\sigma^2$ (bottom row).}
\label{fig:metropolis_results}
\end{figure}


\begin{comment}
We will use the Metropolis algorithm to obtain samples from a multivariate normal distribution to demonstrate this process.
Suppose also that we desire to obtain samples from a multivariate normal distribution with arbitrary covariance matrix $\Sigma$, and that this is difficult (obviously we can do this directly in Python, but this is merely a tutorial to see how the Metropolis algorithm works). Suppose further that we are able to easily compute the ratio of the density of this distribution at two points $\mathbf{x}$ and $\mathbf{y}$ of length $K$, i.e.
\begin{align*}
\frac{N(\mathbf{x} \; ; \; \mu, \Sigma)}{N(\mathbf{y} \; ; \; \mu, \Sigma)} & = \frac{\frac{1}{(2\pi)^{K/2}|\Sigma|^{1/2}} e^{-\frac{1}{2}(\mathbf{x} - \mu)\trp  \Sigma^{-1} (\mathbf{x} - \mu)}}{\frac{1}{(2\pi)^{K/2}|\Sigma|^{1/2}} e^{-\frac{1}{2}(\mathbf{y} - \mu)\trp  \Sigma^{-1} (\mathbf{x} - \mu)}} \\
& = \frac{e^{-\frac{1}{2}(\mathbf{x} - \mu)\trp  \Sigma^{-1} (\mathbf{x} - \mu)}}{e^{-\frac{1}{2}(\mathbf{y} - \mu)\trp  \Sigma^{-1} (\mathbf{x} - \mu)}} \\
& = e^{-\frac{1}{2}\left((\mathbf{x} - \mu)\trp  \Sigma^{-1} (\mathbf{x} - \mu) - (\mathbf{y} - \mu)\trp  \Sigma^{-1} (\mathbf{y} - \mu)\right)}
\end{align*}

\begin{problem}
\label{problem1}
Write an acceptance function that computes
\begin{equation*}
p = \min \{1, e^{-\frac{1}{2}\left((\mathbf{x} - \mu)\trp  \Sigma^{-1} (\mathbf{x} - \mu) - (\mathbf{y} - \mu)\trp  \Sigma^{-1} (\mathbf{y} - \mu)\right)}\}
\end{equation*}
given $\mathbf{x}, \mathbf{y}, \mu,$ and $\Sigma$, and then draws from a Bernoulli distribution with parameter $p$. It should return a $1$ if it accepts the new state, and a $0$ if it rejects it.
\end{problem}

Specifically, we will try to sample from the distribution centered at the origin, with covariance matrix
\begin{equation*}
\Sigma = \left[ \begin{array}{cc} 12 & 4 \\ 4 & 16 \end{array} \right]
\end{equation*}

\begin{lstlisting}
>>> mu = np.zeros(2)
>>> sigma = np.array([[12., 10.], [10., 16.]])
\end{lstlisting}

We will let $Q(\mathbf{x} | \mathbf{y}) = N(\mathbf{x} \; ; \; \mathbf{y}, I)$ be our proposal distribution, given that we are currently in state $\mathbf{y}$, i.e. we propose a new state by drawing from the multivariate normal distribution centered at $\mathbf{y}$ with identity covariance. We then accept according to our acceptance probability, computed in Problem \ref{problem1}.

\begin{problem}
Write a function that accepts a current state, the mean and covariance from the distribution we desire to sample from, and returns the next state. We should propose according to $Q$ described above, and accept according to the function in Problem \ref{problem1}.
\end{problem}

We now have a way to sample a new state from an old state.
As we've stated before, this method creates a Markov chain that \emph{converges} to the desired distribution; at the beginning, however, if our initial guess is highly unlikely for the desired distribution, it may take a while before we get there.
We would like to measure our progress.

\begin{problem}
Write a function that computes the log of the multivariate normal density of a point $\mathbf{x}$ given a mean $\mu$ and covariance matrix $\Sigma$. Be intelligent about how you implement this, that is, do not simply compute the multivariate normal density and then take the log of it, as this may lead to numerical issues. The whole purpose of looking at the multivariate log is to make this more stable.
\end{problem}

We will finally put everything together.

\begin{problem}
Write a function that accepts an initial point $\mathbf{x}$, a mean $\mu$ and covariance $\Sigma$ for the desired sampling distribution, and which performs the Metropolis algorithm for a number of iterations, $n\_samples$. Save each sample $\mathbf{x}$ as produced by the algorithm. Also compute the log of the multivariate normal density of each point, and return both the samples and the logprobs.
\end{problem}

We would like to see how long it takes for our algorithm to converge to the right distribution. We can do this by plotting the log-probs returned by our function. Here we use an initial state $\mathbf{x} = \left[\begin{array}{cc} 100 & 100 \end{array}\right]$.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figures/logprobs.pdf}
\caption{Log probabilities of our samples.}
\end{figure}

From this we can see that after between $300$ and $500$ iterations, we had converged to the correct distribution. We can visualize the path of our sampler by plotting the samples themselves:
\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figures/samples.pdf}
\caption{Samples from the Metropolis algorithm.}
\end{figure}

\begin{problem}
Using $\mu$ and $\Sigma$ as defined previously and using an initial state $\mathbf{x} = \left[ \begin{array}{cc} 1000 & -1000 \end{array} \right]$ run your Metropolis sampler for $10000$ iterations. Plot the log probs as well as the samples. How long did it take to converge?
\end{problem}
\end{comment}

\begin{problem}
Write a function that uses the Metropolis Hastings algorithm to draw from the posterior distribution over the mean $\mu$ and variance $\sigma^2$. Use the given functions and Algorithm \ref{alg:metropolis} to complete the problem.

Your function should return an array of draws, an array of the log probabilities, and an acceptance rate. 
Create plots resembling Figures \ref{fig:logprobs} and \ref{fig:metropolis_results}:
\begin{itemize}
\item Plot the log probabilities of the first 500 samples.
\item Plot the samples for $\mu$ in the order they were drawn, and likewise for $\sigma^2$.
\item Using \li{seaborn.kdeplot} plot the distribution of all samples for $\mu$, and likewise for $\sigma^2$.
\end{itemize}

Use $u=20$ for the parameter of the proposal function.
Use the initial state $\y_0=(\mu_0,\sigma^2_0)=(40,10)$.
Take 10,000 samples for both $\mu$ and $\sigma^2$. 

Use the following code to load the data and initialize the priors:
\begin{lstlisting}
# Load in the data and initialize priors
>>> scores = np.load("examscores.npy")

# Prior sigma^2 ~ IG(alpha, beta)
>>> alpha = 3
>>> beta = 50
>>> muprior = stats.norm(loc=m, scale=sqrt(s**2))

#Prior mu ~ N(m, s)
>>> m = 80
>>> s = 4
>>> sig2prior = stats.invgamma(alpha, scale=beta)
\end{lstlisting}

\end{problem}

\section*{The Ising Model}
In statistical mechanics, the Ising model describes how atoms interact in ferromagnetic material. Assume we have some lattice $\Lambda$ of sites. We say $i \sim j$ if $i$ and $j$ are adjacent sites. Each site $i$ in our lattice is assigned an associated \emph{spin} $\sigma_{i} \in \{\pm 1\}$. A \emph{state} in our Ising model is a particular spin configuration $\sigma = (\sigma_{k})_{k \in \Lambda}$. If $L = |\Lambda|$, then there are $2^{L}$ possible states in our model. If $L$ is large, the state space becomes huge, which is why MCMC sampling methods (in particular the Metropolis algorithm) are so useful in calculating model estimations.

With any spin configuration $\sigma$, there is an associated energy
\[
H(\sigma) = -J \sum_{i \sim j} \sigma_{i} \sigma_{j}
\]
 where $J > 0$ for ferromagnetic materials, and $J < 0$ for antiferromagnetic materials. Throughout this lab, we will assume $J = 1$, leaving the energy equation to be $H(\sigma) = -\sum_{i \sim j} \sigma_{i}\sigma_{j}$ where the interaction from each pair is added only once.

We will consider a lattice that is a $100 \times 100$ square grid.
The adjacent sites for a given site are those directly above, below, to the left, and to the right of the site, so to speak.
For sites on the edge of the grid, we assume it wraps around.
In other words, a site at the farthest left side of the grid is adjacent to the corresponding site on the farthest right side.
Thus, a single spin configuration can be represented as a $100 \times 100$ array, with entries of $\pm 1$.

The following code will construct a random spin configuration of size n:

\begin{lstlisting}
def random_lattice(n):
    """Constructs a random spin configuration for an nxn lattice."""
    random_spin = np.zeros((n,n))
    for k in range(n):
        random_spin[k,:] = 2*np.random.binomial(1,.5, n) -1
    return random_spin
\end{lstlisting}

\begin{figure}[H]
\centering
\includegraphics[width=.5\textwidth]{figures/initial_config.pdf}
\caption{Spin configuration from random initialization.}
\label{fig:random_spin}
\end{figure}

\begin{problem} % Calculate the energy of the spin configuration.
\label{problem2}
Write a function that accepts a spin configuration $\sigma$ for a lattice as a NumPy array.
Compute the energy $H(\sigma)$ of the spin configuration.
Be careful to not double count site pair interactions!
\\(Hint: \li{np.roll()} may be helpful.)
\end{problem}

Different spin configurations occur with different probabilities, depending on the energy of the spin configuration and $\beta > 0$, a quantity inversely proportional to the temperature.
More specifically, for a given $\beta$, we have
\begin{equation*}
\mathbb{P}_{\beta}(\sigma) = \frac{e^{-\beta H(\sigma)}}{Z_{\beta}}
\end{equation*}
where $Z_{\beta} = \sum_{\sigma} e^{-\beta H(\sigma)}$.
Because there are $2^{100 \cdot 100} = 2^{10000}$ possible spin configurations for our particular lattice, computing this sum is infeasible.
However, the numerator is quite simple, provided we can efficiently compute the energy $H(\sigma)$ of a spin configuration.
Thus the ratio of the probability densities of two spin configurations is simple:
\begin{equation*}
\frac{\mathbb{P}_{\beta}(\sigma^{*})}{\mathbb{P}_{\beta}(\sigma)}
= \frac{e^{-\beta H(\sigma^{*})}}{e^{-\beta H(\sigma)}}
= e^{\beta (H(\sigma) - H(\sigma^{*}))}
\end{equation*}

The simplicity of this ratio should lead us to think that a Metropolis algorithm might be an appropriate way by which to sample from the spin configuration probability distribution, in which case the acceptance probability would be
\begin{equation}
A(\sigma^{*}, \sigma) = \begin{cases} 1 & \mbox{if } H(\sigma^{*}) < H(\sigma) \\ e^{\beta (H(\sigma) - H(\sigma^{*}))} & \mbox{ otherwise.} \end{cases}
\label{eq:ising-acceptance}
\end{equation}

By choosing our transition matrix $Q$ cleverly, we can also make it easy to compute the energy for any proposed spin configuration.
We restrict our possible proposals to only those spin configurations in which we have flipped the spin at exactly one lattice site, i.e. we choose a lattice site $i$ and flip its spin.
Thus, there are only $L$ possible proposal spin configurations $\sigma^{*}$ given $\sigma$, each being proposed with probability $\frac{1}{L}$, and such that $\sigma_{j}^{*} = \sigma_{j}$ for all $j \neq i$, and $\sigma_{i}^{*} = - \sigma_{i}$.
Note that we would never actually write out this matrix (it would be $2^{10000} \times 2^{10000}$).
Computing the proposed site's energy is simple: if the spin flip site is $i$, then we have
\begin{equation}
H(\sigma^{*}) = H(\sigma) + 2\sum_{j: j \sim i} \sigma_{i}\sigma_{j}.
\label{eq:ising-new-spin-energy}
\end{equation}

\begin{problem} % Choose somewhere to flip a bit.
Write a function that accepts an integer $n$ and chooses a pair of indices $(i,j)$ where $0 \le i,j \le n-1$.
Each possible pair should have an equal probability $\frac{1}{n^2}$ of being chosen.
\label{prob:ising-flip-site}
\end{problem}

\begin{problem} % Compute the energy of the proposed configuration.
Write a function that accepts a spin configuration $\sigma$, its energy $H(\sigma)$, and integer indices $i$ and $j$.
Use \eqref{eq:ising-new-spin-energy} to compute the energy of the new spin configuration $\sigma^*$, which is $\sigma$ but with the spin flipped at the $(i,j)$th entry of the corresponding lattice.
Do not explicitly construct the new lattice for $\sigma^*$.
\label{prob:ising-new-energy}
\end{problem}

\begin{problem} % Accept / reject the new configuration.
Write a function that accepts a float $\beta$ and spin configuration energies $H(\sigma)$ and $H(\sigma^*)$.
Using \eqref{eq:ising-acceptance}, calculate whether or not the new spin configuration $\sigma^*$ should be accepted (return \li{True} or \li{False}).
Consider doing the calculations in log space.
(Hint: np.random.binomial() might be useful)
\label{prob:ising-acceptance}
\end{problem}

To track the convergence of the Markov chain, we would like to look at the probabilities of each sample at each time. However, this would require us to compute the denominator $Z_{\beta}$, which is generally the reason we have to use a Metropolis algorithm to begin with.
We can get away with examining only $-\beta H(\sigma)$.
We should see this value increase as the algorithm proceeds, and it should converge once we are sampling from the correct distribution.
Note that we don't expect these values to converge to a specific value, but rather to a restricted range of values.

\begin{problem}
Write a function that accepts a float $\beta>0$ and integers $n$, \li{n_samples}, and \li{burn_in}.
Initialize an $n\times n$ lattice for a spin configuration $\sigma$ using Problem \ref{problem2}.
Use the Metropolis algorithm to (potentially) update the lattice \li{burn_in} times.
\begin{enumerate}
    \item Use Problem \ref{prob:ising-flip-site} to choose a site for possibly flipping the spin, thus defining a potential new configuration $\sigma^*$.
    \item Use Problem \ref{prob:ising-new-energy} to calculate the energy $H(\sigma^*)$ of the proposed configuration.
    \item Use Problem \ref{prob:ising-acceptance} to accept or reject the proposed configuration.
    If it is accepted, set $\sigma = \sigma^*$ by flipping the spin at the indicated site.
    \item Track $-\beta H(\sigma)$ at each iteration (independent of acceptance).
\end{enumerate}
After the burn-in period, continue the iteration \li{n_samples} times, also recording every $100$th sample (to prevent memory failure).
The acceptance rate is counted after the burn-in period.
Return the samples, the sequence of weighted energies $-\beta H(\sigma)$, and the acceptance rate.

Test your sampler on a $100 \times 100$ grid with $20{\small,}0000$ total iterations, with \li{n_samples} large enough so that you will keep $50$ samples, for $\beta = 0.2, 0.4, 1$.
Plot the proportional log probabilities across all iterations (including the burn-in), as well as a late sample from each test.
How does the ferromagnetic material behave differently with differing temperatures?
Recall that $\beta$ is an inverse function of temperature.
You should see more structure with lower temperature, as illustrated in Figure \ref{fig:ising-results}.

To show the spin configuration, use \li{plt.imshow(L,cmap='gray')}.
\end{problem}

\begin{figure}
\begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/beta0_2_logprobs.pdf}
    \caption{Proportional log probs when $\beta = 0.2$.}
\end{subfigure}
%
\begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics[width=.62\textwidth]{figures/beta0_2.pdf}
    \caption{Spin configuration sample when $\beta = 0.2$.}
\end{subfigure}
\\
\begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/beta0_4_logprobs.pdf}
    \caption{Proportional log probs when $\beta = 0.4$.}
\end{subfigure}
%
\begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics[width=.62\textwidth]{figures/beta0_4.pdf}
        \caption{Spin configuration sample when $\beta = 0.4$.}
\end{subfigure}
\\
\begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/beta1_logprobs.pdf}
    \caption{Proportional log probs when $\beta = 1$.}
\end{subfigure}
%
\begin{subfigure}{.49\textwidth}
    \centering
    \includegraphics[width=.62\textwidth]{figures/beta1.pdf}
    \caption{Spin configuration sample when $\beta = 1$.}
\end{subfigure}
\caption{}
\label{fig:ising-results}
\end{figure}
