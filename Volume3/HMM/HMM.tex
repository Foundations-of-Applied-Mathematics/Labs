\lab{Discrete Hidden Markov Models}{Discrete Hidden Markov Models}
\label{lab:hmm}
\labdependencies{}
\objective{Understand how to use discrete Hidden Markov Models.}

A common probabilistic model is the \emph{hidden Markov model} (HMM).
In an HMM, we have two sequences of random variables, $(X_t)_{t=0}^\infty$ and $(Z_t)_{t=0}^\infty$.
The $X_t$ are called the \emph{state sequence} or \emph{hidden state}, and the $Z_t$ are known as the \emph{observation sequence}.
We assume that the $X_t$ form a Markov chain, i.e. the distribution of $X_t$ is entirely determined by the value of $X_{t-1}$, and that the distribution of $Z_t$ is determined by the value of $X_t$.
We also typically assume that we only know the values of the $Z_t$, not the $X_t$ (hence the name).
We denote the state space as $\mathscr{X}$ and the observation space as $\mathscr{Z}$, so that for all $t$ we have $X_t\in \mathscr{X}$ and $Z_t\in \mathscr{Z}$.
Hidden Markov models are useful in many situations where we have indirect observations of a sequential or time-based process, including speech and handwriting prediction, text analysis, gene prediction, and many other areas.

In this lab, we explore HMMs with discrete state and observation spaces.
Assume the state space $\mathscr{X}$ and observation space $\mathscr{Z}$ are finite sets where $\vert \mathscr{X}\vert = n$ and $\vert \mathscr{Z} \vert = m$.
For simplicity we relabel these sets as $\mathscr{X}=\{0,1,2,\ldots,n-1\}$ and $\mathscr{Z}=\{0,1,2,\ldots,m-1\}$
We will also assume that the HMM is temporally homogeneous, i.e. that the transition probabilities do not change with $t$.

In this case, we can parameterize all such HMMs by $\boldsymbol\theta = (\boldsymbol\pi, A, B)$ where $\boldsymbol\pi\in\mathbb{R}^n$ represents the distribution of $X_0$ (the initial state distribution), $A$ is a $n \times n$
column-stochastic matrix describing how $X_t$ is affected by $X_{t-1}$ (the state transition matrix), and $B$ is a $m \times n$ column-stochastic matrix describing how $Z_t$ is affected by $X_t$ (the state observation matrix).
The entries of $\boldsymbol\pi$, $A$, and $B$ specifically are the following:
\begin{align*}
\boldsymbol\pi_i &=P(X_0=i) \\
a_{ij} &= P(X_t=i | X_{t-1}=j)
\\
b_{ij} &= P(Z_t=i | X_t=j)
\end{align*}
Finally, we let $\boldsymbol z=[z_0 ,z_1,\ldots z_{T-1}]$ be a vector of observations, where each $z_t$ is a draw from $Z_t$.

Given one or both of $\boldsymbol\theta$ and $\mathbf{z}$, there are several questions we might want to answer:
\begin{enumerate}
 \item What is the likelihood that our model generated the observation sequence? In other words, what is $P(\boldsymbol z | \boldsymbol\theta)$?
 \item Given $\boldsymbol z$, $\boldsymbol\theta$, and an integer $0\leq k \leq T-1$, what is the most likely value for the state $X_k$ at time $k$?
 \item Given $\boldsymbol z$ and $\boldsymbol\theta$, what is the most likely state sequence $\boldsymbol x$ to have generated $\boldsymbol z$?
 \item How can we choose the parameters $\boldsymbol\theta$ that maximize $P(\boldsymbol z | \boldsymbol\theta)$?
\end{enumerate}
The first of these is answered by the \emph{forward pass} algorithm; the second by the \emph{backwards pass} algorithm; the third by the \emph{Viterbi algorithm}; and the fourth is typically approached using the \emph{Baum-Welch algorithm}, which is the special case of expectation maximization applied to an HMM.
Throughout this lab, we will use all four of these algorithms.

\begin{problem}
Create a class called \li{HMM}.
Create the constructor, which accepts arguments \li{pi}, \li{A}, and \li{B}.
Save each of these as an attribute with the same name.
\end{problem}

\subsection*{The Forward Pass}
The goal of the forward pass algorithm is to efficiently compute $P(\boldsymbol z | \boldsymbol\theta)$.
Directly expanding this out as a sum over all state sequences requires a number of computations that grows exponentially with the length of the observation sequence, and is completely impractical.
Instead, the forward pass algorithm splits this probability into separate values that can be easily computed recursively.

As the first step of the algorithm, consider the values
\[
\alpha_t(i)=P(z_0, \ldots, z_t, x_t=i | \boldsymbol\theta).
\]
The law of total probability gives us that
\[
P(\boldsymbol z | \boldsymbol\theta) = \sum_{i \in \mathscr{X}} \alpha_{T-1}(i),
\]
so finding the $\alpha_t(i)$ lets us find $P(\boldsymbol z | \boldsymbol\theta)$.
It can be shown that\footnote{For verification of the mathematics behind this and the other algorithms in this lab, refer to the Volume 3 textbook.}
\begin{align*}
\alpha_0(i) &= \pi_i b_{z_0,i} \\
\alpha_{t}(i) &= b_{z_t,i} \sum_{j\in \mathscr{X}} \alpha_{t-1}(j)a_{ij}
\end{align*}
which allows us to efficiently compute the $\alpha_t(i)$ iteratively.
When we implement this algorithm, we will store the values of $\alpha_t(i)$ in a single 2D array.
The $(t,i)$-th entry of this array will be the value $\alpha_t(i)$.

\begin{problem}\label{hmm:prob:forward}
Create a method \li{forward_pass} in your HMM class to implement the forward pass algorithm.
This function should accept the observation sequence $\mathbf{z}$ (with shape \li{(T,)}) and return the array of $\alpha_t(i)$ values (with shape \li{(T,n)}).

To test your code, use the following example HMM:
\begin{lstlisting}
>>> pi = np.array([.6, .4])
>>> A = np.array([[.7, .4],[.3, .6]])
>>> B = np.array([[.1,.7],[.4, .2],[.5, .1]])
>>> z_example = np.array([0, 1, 0, 2])
>>> example_hmm = HMM(pi, A, B)
\end{lstlisting}
You should get the following output using the example HMM:
\begin{lstlisting}
>>> alpha = example_hmm.forward_pass(z_example)
>>> print(np.sum(alpha[-1, :]) # the probability of the observation
0.009629599999
\end{lstlisting}
\end{problem}

\begin{problem}\label{hmm:prob:stocks1}
Consider the following (very simplified) model of the price of a stock over time as an HMM.
The observation states will be the change in the value of the stock.
For simplicity, we will group these into five values: large decrease, small decrease, no change, small increase, large increase, labeled as integers from 0 to 4.
The hidden state will be the overall trends of the market.
We'll consider the market to have three possible states: declining in value (bear market), not changing in value (stagnant), and increasing in value (bull market), labeled as integers from 0 to 2.
Let the HMM modeling this scenario have parameters
\[
\boldsymbol\pi=\begin{bmatrix}
1/3 \\ 1/3 \\ 1/3
\end{bmatrix},
\quad
A=\begin{bmatrix}
0.5 & 0.3 & 0 \\
0.5 & 0.3 & 0.3 \\
0 & 0.4 & 0.7
\end{bmatrix},
\quad
B=\begin{bmatrix}
0.3 & 0.1 & 0 \\
0.3 & 0.2 & 0.1 \\
0.3 & 0.4 & 0.3 \\
0.1 & 0.2 & 0.4 \\
0 & 0.1 & 0.2
\end{bmatrix}
\]
The file \li{stocks.npy} contains a sequence of 50 observations drawn from this HMM.
What is the probability of this observation sequence given these model parameters?
Use your implementation of the forward pass algorithm from Problem \ref{hmm:prob:forward} to find the answer.
Note that the answer is very small, because there are lots of possible observation sequences.
\end{problem}

\subsection*{The Backward Pass}
The backward pass algorithm seeks to answer the second question: given an observation sequence, parameters for an HMM, and a specific timestep, what is the most likely state at that step?
As with the first question, trying to directly compute the answer via expanding into a sum over individual terms is completely impractical.
The backwards pass algorithm takes a different approach.

Define the function
\[
\gamma_t(i)=P(X_t=i|\mathbf{z},\boldsymbol\theta).
\]
The answer to the second question is then given by $\argmax_{i\in\mathscr{X}}\gamma_t(i)$ for a fixed timestep $t$.
In order to compute the $\gamma_t(i)$ efficiently, the backwards pass breaks the problem up in a clever way to allow an efficient iterative solution.
Consider the values
\[
\beta_t(j)=P(z_{t+1},z_{t+2},\ldots,z_{T-1}|X_t=j,\boldsymbol\theta).
\]
where $\beta_{T-1}(i)=1$.
It can be shown that
\[
\beta_t(j)
=\sum_{i\in\mathscr{X}} a_{ij}\beta_{t+1}(i)b_{z_{t+1},i}
\]
and that
\[
\gamma_t(i)=\frac{\alpha_t(i)\beta_t(i)}{P(\mathbf{z}|\boldsymbol\theta)},
\]
allowing efficient computation of these values.
The backwards pass algorithm simply consists of iteratively computing the $\beta_t(i)$ values and using those to compute $\gamma_t(i)$.
Note that the backwards pass algorithm requires running the forward pass algorithm first, and iterates over $t$ in the opposite order.

\begin{problem}\label{hmm:prob:backwards}
Create a method \li{backward_pass} in your HMM class to implement the backward pass algorithm.
This function should accept the observation sequence $\mathbf{z}$ (with shape \li{(T,)}) and return two arrays of the $\beta_t(i)$ and $\gamma_t(i)$ values (each with shape \li{(T,n)}).

To test your function, your code should produce the following output on the example HMM:
\begin{lstlisting}
>>> beta, gamma = example_hmm.backward_pass(z_example, alpha)
>>> print(beta)
[[0.0302  0.02792]
 [0.0812  0.1244 ]
 [0.38    0.26   ]
 [1.      1.     ]]
>>> print(gamma)
[[0.18816981 0.81183019]
 [0.51943175 0.48056825]
 [0.22887763 0.77112237]
 [0.8039794  0.1960206 ]]
\end{lstlisting}

With your function and the stock model from Problem \ref{hmm:prob:stocks1}, answer the following question: given the observation sequence in \li{stocks.npy}, what is the most likely initial hidden state $X_0$?
\end{problem}

\subsection*{Most Likely Sequence with the Viterbi Algorithm}

The Viterbi algorithm is a dynamic programming algorithm that seeks to find the most likely sequence of hidden states $\x=(x_0,\ldots,x_{T-1})$ that satisfies
\[
\x^*=\underset{\x}{\argmax} \;P(\x|\mathbf{z},\boldsymbol\theta).
\]
The algorithm proceeds by considering the values
\[
\eta_t(i)
=
\max_{x_0,\ldots, x_{t-1}}
P(x_0,\ldots,x_{t-1}, X_t=i, z_0,\ldots,z_t|\boldsymbol\theta)
\]
The Bellman optimality principal can be used to show that
\begin{align*}
\eta_0(i)&= b_{z_0,i}\boldsymbol\pi_i, \\
\eta_t(i)&=\max_{j\in\mathscr{X}}  b_{z_t,i}a_{ij}\eta_{t-1}(j),
\end{align*}
allowing us to compute these efficiently.
The $\eta_t(i)$ give the maximizing probabilities at each timestep assuming we are in a certain hidden state.
To extract the most likely sequence from the $\eta_t(i)$s, we iterate backwards as follows:
\begin{align*}
x^*_{T-1}&=\underset{j\in\mathscr{X}}{\argmax}\;\eta_{T-1}(j) \\
x^*_{t-1} &=
\underset{j\in\mathscr{X}}{\argmax}\;
b_{z_t,x^*_t}a_{x^*_t,j}\eta_{t-1}(j)
=
\underset{j\in\mathscr{X}}{\argmax}\;
a_{x^*_t,j}\eta_{t-1}(j).
\end{align*}

\begin{problem}
Creating a method \li{viterbi_algorithm} in your HMM class to implement the Viterbi algorithm.
This function should accept the observation sequence $\mathbf{z}$ (with shape \li{(T,)}) and return the most likely state sequence $\x^*$ (as an array with shape \li{(T,)}).

To test your function, it should output the following on the example HMM:
\begin{lstlisting}
>>> xstar = example_hmm.viterbi_algorithm(z_example)
>>> print(xstar)
[1 1 1 0]
\end{lstlisting}
Apply your function to the stock market HMM from Problem \ref{hmm:prob:stocks1}.
With the observaition sequence from \li{stocks.npy}, what is the most likely sequence of hidden states?
Is the initial state of the most likely sequence the same as the most likely initial state you found in Problem \ref{hmm:prob:backwards}?
\end{problem}

\subsection*{Text Analysis with HMMs}
We now turn to an interesting application of the Baum-Welch algorithm to train HMMs.
We have coded most of the pieces needed for the Baum-Welch algorithm already in this lab.
However, many of these require the computations to be done in a more careful way than we have presented here in order to prevent underflow.
Instead of delving into the details of that, we will use the HMM implementation provided by the \li{hmmlearn} package, specifically the \li{hmmlearn.hmm.CategoricalHMM} class.

This class uses slightly different conventions and syntax from the HMM class that we have coded in this lab, so we will illustrate how to use this class on the data from \li{stocks.npy}.
\begin{lstlisting}
import numpy as np
from hmmlearn import hmm
z = np.load("stocks.npy")
\end{lstlisting}
In the initializer, we specify the number of hidden states as the \li{n_components} argument (we will use 3):
\begin{lstlisting}
h = hmm.CategoricalHMM(n_components=3)
\end{lstlisting}
To train the HMM, call the \li{fit} method.
This method accepts the observation sequence $\mathbf{z}$.
However, it expects it to be an array with shape \li{(T,1)}, so reshape it as follows when you pass it in:
\begin{lstlisting}
h.fit(z.reshape(-1, 1))
\end{lstlisting}
Now the HMM is trained.
To see the trained HMM parameters, use the following attributes:
$\boldsymbol\pi$ is stored as \li{h.startprob_}, $A$ is stored as \li{h.transmat_}, and $B$ is stored as \li{h.emissionprob_}.
However, $A$ and $B$ are transposed from the convention we are using, so extracting the parameters looks like the following:
\begin{lstlisting}
pi = h.startprob_
A = h.transmat_.T
B = h.emissionprob_.T
\end{lstlisting}

The particular application we will use this for is to take some text and treat it as the observation sequence $\mathbf{z}$ of an HMM.
Using the Baum-Welch algorithm to train an HMM with this setup can reveal interesting information about the underlying text.
We will specifically use the sequence of characters (after stripping out punctuation and converting everything to lower-case) as our observation sequence.

In order to convert the raw text into data we can use with the \li{hmmlearn} package, we need to read and process the text and then map the characters to integer values.
The following code accomplishes this task:
\begin{lstlisting}
import numpy as np
import string
import codecs

def vec_translate(a, my_dict):
    # translate numpy array from symbols to state numbers or vice versa
    return np.vectorize(my_dict.__getitem__)(a)

def prep_data(filename):
    """
    Reads in the file and prepares it for use in an HMM.
    Returns:
        symbols (dict): a dictionary that maps characters to their integer values
        obs_sequence (ndarray): an array of integers representing the read-in text
    """
    # Get the data as a single string
    with codecs.open(filename, encoding="utf-8") as f:
        data=f.read().lower()  # and convert to all lower case
    # remove punctuation and newlines
    remove_punct_map = {ord(char):
                        None for char in string.punctuation+"\n\r"}
    data = data.translate(remove_punct_map)
    # make a list of the symbols in the data
    symbols = sorted(list(set(data)))
    # convert the data to a NumPy array of symbols
    a = np.array(list(data))
    # make a conversion dictionary from symbols to state numbers
    symbols_to_obsstates = {x: i for i, x in enumerate(symbols)}
    # convert the symbols in a to state numbers
    obs_sequence = vec_translate(a,symbols_to_obsstates)
    return symbols, obs_sequence.reshape(-1, 1)
\end{lstlisting}

\begin{problem}\label{hmm:prob:declaration}
The file \li{declaration.txt} contains the text of the Declaraction of Independence.

Train an \li{hmmlearn.hmm.CategoricalHMM} on this data with $N=2$ states and $M=\text{\li{len(set(obs))}}=27$ observation values (26 lower case characters and 1 whitespace character).
Train the HMM with \li{n_iter=200} and \li{tol=1e-4} (note that both of these are arguments to the constructor, not to the \li{fit} function).

Once the learning algorithm converges, analyze the state observation matrix $B$.
Note which rows correspond to the largest and smallest probability values in each column of $B$,
and check the corresponding characters.
The code below displays typical results for a well-converged HMM.
(Note that the \li{u} before the \li{"} indicates that the string should be unicode, which will be useful in the next problem.)
\begin{lstlisting}
>>> B = h.emissionprob_.T
>>> for i in range(len(B)):
>>>     print(u"{}, {:0.4f}, {:0.4f}"
                .format(symbols[i], *B[i,:]))
 , 0.0051, 0.3324
a, 0.0000, 0.1247
c, 0.0460, 0.0000
b, 0.0237, 0.0000
e, 0.0000, 0.2245
d, 0.0630, 0.0000
g, 0.0325, 0.0000
f, 0.0450, 0.0000
i, 0.0000, 0.1174
# ...
\end{lstlisting}
What do you notice about the columns of $B$?
(Hint: Look at the vowels).
Write your observations in a markdown cell.
If there is nothing apparent in your output, try re-running your HMM.
Note that the order of the columns is completely arbitrary, and your code may switch the role of the two columns.
\end{problem}

\begin{comment}
\begin{problem}
Repeat the previous calculation with 3 hidden states.
Interpret/explain your results.
Note that as the number of hidden states increases, the divisions of the characters will become more abstract and less interpretable.

With 3 hidden states, you can use the following to display the matrix $B$:
\begin{lstlisting}
print(u"{}, {:0.4f}, {:0.4f}, {:0.4f}"
        .format(symbols[i], *hmm_example.B[i,:]))
\end{lstlisting}
\end{problem}
\end{comment}

\begin{problem}
The file \li{WarAndPeace.txt} contains a portion of the Russian text of \textit{War and Peace} by Tolstoy.
Train an HMM on the text in this file with $N=2$ states as in Problem \ref{hmm:prob:declaration}.
Interpret/explain your results.
Which Cyrillic characters appear to be vowels?
\end{problem}
